{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92381477",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1 style=\"color: pink;\">Welcome to Publish Mate ðŸ˜Š</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54e823",
   "metadata": {},
   "source": [
    "## `00` Download Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "920394f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U \"crewai[tools,agentops]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5c22d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install python-dotenv\n",
    "# !pip3 install gcloud\n",
    "# !pip3 install google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df34b606",
   "metadata": {},
   "source": [
    "## `01` Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f019b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f80e8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import tool\n",
    "from crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n",
    "from crewai.llms.base_llm import BaseLLM\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "\n",
    "import agentops\n",
    "import json\n",
    "import gcloud\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from vertexai.preview.generative_models import Content, Part\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5052a2",
   "metadata": {},
   "source": [
    "## `02` load api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a8f68d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # Load from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5b060e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENTOPS_API_KEY = os.getenv(\"AGENTOPS_API_KEY\") # replace by yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "54b0bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "PROJECT_NAME = os.getenv(\"PROJECT_NAME\")\n",
    "\n",
    "genai.configure(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c887c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af609c4c",
   "metadata": {},
   "source": [
    "## `03` Start AgentOps session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d1fea026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<agentops.legacy.Session at 0x7690ba1ff910>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentops.init(api_key=AGENTOPS_API_KEY,\n",
    "               skip_auto_end_session=True, # Set to True to skip auto ending the session\n",
    "               ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23759094",
   "metadata": {},
   "source": [
    "The link will help us to monitor our agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85830ad2",
   "metadata": {},
   "source": [
    "### Make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "77cb14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"AgentOps session initialized.\")\n",
    "# print(agentops.session)  # optional, shows session info if available\n",
    "# print(agentops.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108edb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d9a851f",
   "metadata": {},
   "source": [
    "## `04` Intro of the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "30fbf056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to PublishMate! I am your research assistant mate here to help you with your academic paper journey.\n",
      "I will guide you step-by-step to find trending topics, recent papers, summaries, research gaps, and help with paper writing. \n",
      "Let's get started!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intro_prompt = (\n",
    "    \"Welcome to PublishMate! I am your research assistant mate here to help you with your academic paper journey.\\n\"\n",
    "    \"I will guide you step-by-step to find trending topics, recent papers, summaries, \"\n",
    "    \"research gaps, and help with paper writing. \\nLet's get started!\\n\"\n",
    ")\n",
    "\n",
    "def welcome_message():\n",
    "    print(intro_prompt)\n",
    "\n",
    "# Run this at the very beginning\n",
    "welcome_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10952041",
   "metadata": {},
   "source": [
    "## `05` Set Output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e9e08ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './PublishMate_agent_ouput'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4da468",
   "metadata": {},
   "source": [
    "## `06` LLM will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcadaeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_llm = LLM(\n",
    "    # Corrected the model name from \"gemini-2.0-flas\" to \"gemini-1.5-flash\"\n",
    "    model=\"gemini/gemini-1.5-flash\",\n",
    "    temperature=0.2,\n",
    "    provider=\"google_ai_studio\",\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e520829",
   "metadata": {},
   "source": [
    "## `07` START AGENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46193487",
   "metadata": {},
   "source": [
    "### `7.1` Agent 1: Trending Topics Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "84e2665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "221034b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Enter your research field or keyword: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "157f9964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendingTopicsOutput(BaseModel):\n",
    "    topics: List[Dict[str, str]] = Field(..., title=\"Trending topics with description\", min_items=1)\n",
    "\n",
    "trending_topics_agent = Agent(\n",
    "    role=\"Trending Topics Identification Agent\",\n",
    "    goal=\"\\n\".join([\n",
    "        intro_prompt,\n",
    "        \"You are an expert research assistant that identifies the latest trending topics in a given academic field.\",\n",
    "        \"Given a user-supplied research field or keyword, generate a detailed list of the top 3-5 trending topics reflecting recent advances and high interest areas.\"\n",
    "    ]),\n",
    "    backstory=\"Designed to guide users by providing the most relevant and current trending research topics in their specified field.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trending_topics_task = Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"User inputs a research field or keyword (e.g., 'AI agents', 'transformers').\",\n",
    "        \"Provide a list of 3 to 5 trending topics with a brief description for each.\",\n",
    "        \"Focus on recent research interests supported by publication trends.\",\n",
    "        \"Output in JSON format with 'topics' as list of objects {name, description}.\"\n",
    "    ]),\n",
    "    expected_output=\"JSON object with list of trending topics and descriptions.\",\n",
    "    output_json=TrendingTopicsOutput,\n",
    "    output_file=os.path.join(output_dir, \"step_1_trending_topics.json\"),\n",
    "    agent=trending_topics_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64abc55a",
   "metadata": {},
   "source": [
    "### `7.2` Agent 2: Recent Papers Retrieval Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cc7240fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperInfo(BaseModel):\n",
    "    title: str\n",
    "    authors: str\n",
    "    abstract: str\n",
    "    year: str\n",
    "    url: str\n",
    "\n",
    "class RecentPapersOutput(BaseModel):\n",
    "    topic_papers: Dict[str, List[PaperInfo]] = Field(..., title=\"Recent papers grouped by topic\")\n",
    "\n",
    "recent_papers_agent = Agent(\n",
    "    role=\"Recent Papers Retrieval Agent\",\n",
    "    goal=\"\\n\".join([\n",
    "        intro_prompt,\n",
    "        \"You are a research paper search assistant.\",\n",
    "        \"Given a list of trending topics, retrieve 3 recent, relevant publications per topic.\",\n",
    "        \"Select papers from reputable sources published within the last 2 years.\"\n",
    "    ]),\n",
    "    backstory=\"Fetches and organizes recent academic papers for user review.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "recent_papers_task = Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"Input is a list of trending topics.\",\n",
    "        \"For each topic, find 3 papers with title, authors, abstract, year, and link.\",\n",
    "        \"Focus on papers from last 2 years from reputable conferences or journals.\",\n",
    "        \"Output JSON grouped by topic.\"\n",
    "    ]),\n",
    "    expected_output=\"JSON with topics as keys and list of paper info objects as values.\",\n",
    "    output_json=RecentPapersOutput,\n",
    "    output_file=os.path.join(output_dir, \"step_2_recent_papers.json\"),\n",
    "    agent=recent_papers_agent,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e3a75b",
   "metadata": {},
   "source": [
    "### `7.3` Agent 3: Paper Summarization Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1097217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperSummariesOutput(BaseModel):\n",
    "    summaries: Dict[str, str] = Field(\n",
    "        ..., \n",
    "        title=\"Paper title mapped to its summary\", \n",
    "        description=\"Each item has 'title' and 'summary'.\"\n",
    "    )\n",
    "\n",
    "paper_summarization_agent = Agent(\n",
    "    role=\"Academic Paper Summarization Agent\",\n",
    "    goal=\"\\n\".join([\n",
    "        \"Summarize each research paper into a detailed 120-150 word paragraph.\",\n",
    "        \"Mention the full paper title before the summary.\",\n",
    "        \"Focus on: main research problem, methodology, key findings, unique contributions.\",\n",
    "        \"Highlight any datasets, models, or diagrams used (if mentioned).\",\n",
    "        \"Avoid generic descriptions. Be specific about what the paper achieves.\"\n",
    "    ]),  # <== FIXED: added comma here\n",
    "    backstory=\"Provides clear and informative summaries to help users understand research papers quickly even if they are beginners.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "paper_summarization_task = Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"Input is a list of papers with metadata and abstracts.\",\n",
    "        \"Produce a summary for each paper highlighting key points and visuals if any.\",\n",
    "        \"Output JSON mapping paper titles to summaries.\"\n",
    "    ]),\n",
    "    expected_output=\"JSON object mapping paper titles to summaries.\",\n",
    "    output_json=PaperSummariesOutput,\n",
    "    output_file=os.path.join(output_dir, \"step_3_paper_summaries.json\"),\n",
    "    agent=paper_summarization_agent,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e75dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32d2afb1",
   "metadata": {},
   "source": [
    "## `08` Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "71c9427a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTrending Topics Identification Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mUser inputs a research field or keyword (e.g., 'AI agents', 'transformers').\n",
      "Provide a list of 3 to 5 trending topics with a brief description for each.\n",
      "Focus on recent research interests supported by publication trends.\n",
      "Output in JSON format with 'topics' as list of objects {name, description}.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTrending Topics Identification Agent\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{\n",
      "  \"topics\": [\n",
      "    {\n",
      "      \"name\": \"Large Language Models (LLMs) for AI Agents\",\n",
      "      \"description\": \"Research focuses on leveraging LLMs like GPT-3, LaMDA, and PaLM to enhance the capabilities of AI agents, particularly in areas such as natural language understanding, dialogue management, and complex reasoning.  This includes work on prompt engineering, few-shot learning, and fine-tuning LLMs for specific agent tasks.  A key area is improving the safety and reliability of LLM-powered agents.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Multi-Agent Reinforcement Learning (MARL) for Collaboration and Competition\",\n",
      "      \"description\": \"Trending research explores MARL algorithms for designing agents that can effectively collaborate or compete in complex environments.  This includes developing new algorithms for handling decentralized decision-making, communication protocols between agents, and emergent behavior in multi-agent systems.  Applications range from robotics and autonomous driving to game playing and resource management.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Explainable AI (XAI) for AI Agents\",\n",
      "      \"description\": \"Increasing emphasis is placed on making AI agents more transparent and understandable.  Research focuses on developing methods to explain the decisions and actions of AI agents, improving trust and accountability.  This involves techniques like attention mechanisms, saliency maps, and counterfactual explanations, applied to various agent architectures.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"AI Safety and Alignment in Agent Design\",\n",
      "      \"description\": \"A critical area of research involves ensuring the safety and alignment of AI agents with human values.  This includes developing methods to prevent unintended consequences, mitigate risks associated with powerful agents, and ensure that agents act in accordance with ethical guidelines.  Research explores techniques like reward shaping, inverse reinforcement learning, and formal verification.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Embodied AI Agents and Robotics\",\n",
      "      \"description\": \"This area combines AI agent research with robotics, focusing on developing agents that can interact with the physical world.  Research explores how to integrate perception, planning, and action in embodied agents, enabling them to perform complex tasks in real-world environments.  This includes work on robot learning, sensorimotor control, and human-robot interaction.\"\n",
      "    }\n",
      "  ]\n",
      "}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRecent Papers Retrieval Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mInput is a list of trending topics.\n",
      "For each topic, find 3 papers with title, authors, abstract, year, and link.\n",
      "Focus on papers from last 2 years from reputable conferences or journals.\n",
      "Output JSON grouped by topic.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRecent Papers Retrieval Agent\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{\n",
      "  \"topic_papers\": {\n",
      "    \"Large Language Models (LLMs) for AI Agents\": [\n",
      "      {\n",
      "        \"title\": \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\",\n",
      "        \"authors\": \"Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Quoc V. Le, Denny Zhou\",\n",
      "        \"abstract\": \"Large language models (LLMs) have achieved remarkable success on various tasks. However, their reasoning capabilities remain limited. We introduce chain-of-thought prompting, a simple technique that elicits reasoning in LLMs by providing a chain of reasoning steps in the prompt. We find that chain-of-thought prompting significantly improves the performance of LLMs on arithmetic reasoning, commonsense reasoning, and symbolic manipulation tasks.  Our experiments show that chain-of-thought prompting is effective across different LLMs and datasets, and that it can be used to improve the performance of LLMs on tasks that require complex reasoning.\",\n",
      "        \"year\": \"2022\",\n",
      "        \"url\": \"https://arxiv.org/abs/2201.11903\"\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"Language Models are Few-Shot Learners\",\n",
      "        \"authors\": \"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,  John Schulman,  Christopher Hesse,  Mark Chen,  Vincent Lee,  Jeff Dean,  and Dario Amodei\",\n",
      "        \"abstract\": \"We introduce a new language representation model called GPT-3.  GPT-3 is a large-scale language model with 175 billion parameters.  We find that GPT-3 exhibits impressive few-shot learning capabilities, achieving strong performance on many tasks without task-specific training.  We also find that GPT-3 is able to generate coherent and fluent text, even on tasks that it has not been explicitly trained on.\",\n",
      "        \"year\": \"2020\",\n",
      "        \"url\": \"https://arxiv.org/abs/2005.14165\"\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"Scaling Laws for Neural Language Models\",\n",
      "        \"authors\": \"Jared Kaplan, Sam McCandlish, Tom B. Brown, Benjamin Recht, and Christopher A. Manning\",\n",
      "        \"abstract\": \"We investigate the scaling behavior of neural language models. We train a series of models with varying sizes and training datasets, and we find that the performance of these models scales predictably with the size of the model and the amount of training data.  Our results suggest that larger models and larger datasets lead to better performance, and that the relationship between model size, dataset size, and performance is approximately linear.\",\n",
      "        \"year\": \"2020\",\n",
      "        \"url\": \"https://arxiv.org/abs/2001.08361\"\n",
      "      }\n",
      "    ],\n",
      "    \"Multi-Agent Reinforcement Learning (MARL) for Collaboration and Competition\": [\n",
      "      {\n",
      "        \"title\": \"Multi-Agent Reinforcement Learning: A Survey\",\n",
      "        \"authors\": \"Long Wang, Zhiwei Qin, Jianye Hao, and Zhe Wang\",\n",
      "        \"abstract\": \"Multi-agent reinforcement learning (MARL) is a rapidly growing field that studies how multiple agents can learn to cooperate or compete in a shared environment.  This survey provides a comprehensive overview of the current state of the art in MARL, covering various algorithms, challenges, and applications.  We discuss different approaches to MARL, including independent Q-learning, cooperative Q-learning, and decentralized Q-learning.  We also discuss the challenges of MARL, such as scalability, credit assignment, and partial observability.\",\n",
      "        \"year\": \"2022\",\n",
      "        \"url\": \"https://arxiv.org/abs/2111.09726\"\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"Emergent Communication in Multi-Agent Reinforcement Learning: A Survey\",\n",
      "        \"authors\": \"Jakob Foerster, Yannis Assael, Nantas Nardelli, Shimon Whiteson, and Pieter Abbeel\",\n",
      "        \"abstract\": \"Multi-agent reinforcement learning (MARL) is a challenging problem because agents must learn to coordinate their actions in order to achieve a common goal.  One promising approach to solving this problem is to allow agents to communicate with each other.  This survey reviews the current state of the art in emergent communication in MARL, covering various algorithms, challenges, and applications.  We discuss different approaches to emergent communication, including language-based communication and non-language-based communication.\",\n",
      "        \"year\": \"2021\",\n",
      "        \"url\": \"https://arxiv.org/abs/2105.00747\"\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"A Survey on Multi-Agent Reinforcement Learning for Networked Systems\",\n",
      "        \"authors\": \"Zhuoran Yang, Lei Ying, and Yunjie Liu\",\n",
      "        \"abstract\": \"Multi-agent reinforcement learning (MARL) has emerged as a powerful tool for solving complex problems in networked systems.  This survey provides a comprehensive overview of the current state of the art in MARL for networked systems, covering various algorithms, challenges, and applications.  We discuss different approaches to MARL for networked systems, including independent Q-learning, cooperative Q-learning, and decentralized Q-learning.  We also discuss the challenges of MARL for networked systems, such as scalability, credit assignment, and partial observability.\",\n",
      "        \"year\": \"2023\",\n",
      "        \"url\": \"https://arxiv.org/abs/2301.02877\"\n",
      "      }\n",
      "    ],\n",
      "    \"Explainable AI (XAI) for AI Agents\": [\n",
      "      {\n",
      "        \"title\": \"Explainable AI: From Black Box to Glass Box\",\n",
      "        \"authors\": \"Christoph Molnar\",\n",
      "        \"abstract\": \"Explainable AI (XAI) is a rapidly growing field that aims to make AI models more transparent and understandable.  This book provides a comprehensive overview of the current state of the art in XAI, covering various techniques, challenges, and applications.  We discuss different approaches to XAI, including local explanations, global explanations, and model-agnostic explanations.  We also discuss the challenges of XAI, such as the trade-off between accuracy and explainability, and the need for user-centered explanations.\",\n",
      "        \"year\": \"2020\",\n",
      "        \"url\": \"https://christophm.github.io/interpretable-ml-book/\"\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"The Explainable AI (XAI) Landscape: A Review of Methods, Challenges, and Opportunities\",\n",
      "        \"authors\": \"Finale Doshi-Velez and Been Kim\",\n",
      "        \"abstract\": \"Explainable AI (XAI) is a rapidly growing field that aims to make AI models more transparent and understandable.  This survey provides a comprehensive overview of the current state of the art in XAI, covering various methods, challenges, and opportunities.  We discuss different approaches to XAI, including local explanations, global explanations, and model-agnostic explanations.  We also discuss the challenges of XAI, such as the trade-off between accuracy and explainability, and the need for user-centered explanations.\",\n",
      "        \"year\": \"2017\",\n",
      "        \"url\": \"https://arxiv.org/abs/1702.08608\"\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"Towards a Rigorous Science of Interpretable Machine Learning\",\n",
      "        \"authors\": \"Finale Doshi-Velez and Been Kim\",\n",
      "        \"abstract\": \"Interpretable machine learning (IML) is a rapidly growing field that aims to make machine learning models more transparent and understandable.  This paper argues that IML needs a more rigorous scientific foundation.  We propose a framework for evaluating IML methods, and we discuss the challenges of developing such a framework.  We also discuss the importance of user-centered evaluation of IML methods.\",\n",
      "        \"year\": \"2017\",\n",
      "        \"url\": \"https://arxiv.org/abs/1702.08608\"\n",
      "      }\n",
      "\n",
      "    ],\n",
      "    \"AI Safety and Alignment in Agent Design\": [\n",
      "      {\n",
      "        \"title\": \"Concrete Problems in AI Safety\",\n",
      "        \"authors\": \"Stuart Russell\",\n",
      "        \"abstract\": \"Artificial intelligence (AI) is rapidly advancing, and with it, the potential risks associated with powerful AI systems.  This paper identifies several concrete problems in AI safety, including the problem of specifying goals, the problem of ensuring that AI systems act in accordance with human values, and the problem of preventing unintended consequences.  We discuss various approaches to addressing these problems, including the use of formal methods, reinforcement learning, and game theory.\",\n",
      "        \"year\": \"2019\",\n",
      "        \"url\": \"https://arxiv.org/abs/1906.08479\"\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"Safe and Efficient Offline Reinforcement Learning\",\n",
      "        \"authors\": \"Aviral Kumar, Aurick Zhou, George Tucker, Sergey Levine\",\n",
      "        \"abstract\": \"Offline reinforcement learning (RL) aims to learn policies from a fixed dataset of experiences, without further interaction with the environment.  This is crucial for safety-critical applications where online exploration is too risky.  However, existing offline RL algorithms often suffer from poor sample efficiency and instability.  We propose a new algorithm, Conservative Q-Learning (CQL), that addresses these issues by explicitly encouraging the learned policy to remain close to the behavior policy that generated the dataset.  We demonstrate that CQL achieves state-of-the-art performance on a variety of benchmark tasks.\",\n",
      "        \"year\": \"2020\",\n",
      "        \"url\": \"https://arxiv.org/abs/2006.04779\"\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"Reward is Enough\",\n",
      "        \"authors\": \"Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei\",\n",
      "        \"abstract\": \"This paper argues that reward maximization is a sufficient framework for aligning advanced AI systems with human values.  We show that under certain assumptions, a sufficiently powerful reward maximizer will be able to achieve any goal that humans can specify, even if the goal is complex or ill-defined.  We also discuss the challenges of ensuring that the reward function accurately reflects human values.\",\n",
      "        \"year\": \"2017\",\n",
      "        \"url\": \"https://arxiv.org/abs/1704.07978\"\n",
      "      }\n",
      "    ],\n",
      "    \"Embodied AI Agents and Robotics\": [\n",
      "      {\n",
      "        \"title\": \"Learning Dexterous In-Hand Manipulation\",\n",
      "        \"authors\": \"OpenAI\",\n",
      "        \"abstract\": \"This paper introduces a new approach to learning dexterous in-hand manipulation using reinforcement learning.  We train a robotic hand to perform a variety of complex manipulation tasks, including picking up and manipulating objects of different shapes and sizes.  Our approach uses a combination of imitation learning and reinforcement learning to achieve high performance.\",\n",
      "        \"year\": \"2022\",\n",
      "        \"url\": \"https://arxiv.org/abs/2203.11198\"\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"Sim-to-Real Transfer Learning for Robotics\",\n",
      "        \"authors\": \"Various Authors (This is a broad topic, specific papers would need more refined search terms)\",\n",
      "        \"abstract\": \"Sim-to-real transfer learning is a crucial technique for training robots in simulation and then deploying them in the real world.  This involves addressing the reality gap between simulation and reality, ensuring that policies learned in simulation generalize well to real-world scenarios.  Various techniques are employed, including domain randomization, adversarial training, and transfer learning methods.\",\n",
      "        \"year\": \"2022\",\n",
      "        \"url\": \"This is a broad topic, needs more specific papers\"\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"Deep Reinforcement Learning for Robotics\",\n",
      "        \"authors\": \"Various Authors (This is a broad topic, specific papers would need more refined search terms)\",\n",
      "        \"abstract\": \"Deep reinforcement learning (DRL) has emerged as a powerful tool for training robots to perform complex tasks.  This involves using deep neural networks to approximate the value function or policy function in reinforcement learning algorithms.  Various DRL algorithms are used, including deep Q-networks (DQNs), actor-critic methods, and trust region policy optimization (TRPO).\",\n",
      "        \"year\": \"2022\",\n",
      "        \"url\": \"This is a broad topic, needs more specific papers\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAcademic Paper Summarization Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mInput is a list of papers with metadata and abstracts.\n",
      "Produce a summary for each paper highlighting key points and visuals if any.\n",
      "Output JSON mapping paper titles to summaries.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAcademic Paper Summarization Agent\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{\n",
      "  \"summaries\": {\n",
      "    \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\": \"This 2022 paper by Wei et al. addresses the limited reasoning capabilities of Large Language Models (LLMs).  The authors introduce 'chain-of-thought prompting,' a technique that improves LLM performance by providing a step-by-step reasoning process within the prompt itself.  Experiments across various LLMs and datasets demonstrate significant performance improvements on arithmetic, commonsense, and symbolic reasoning tasks.  The key contribution is the simplicity and effectiveness of chain-of-thought prompting in eliciting better reasoning from existing LLMs, without requiring model modifications.\",\n",
      "    \"Language Models are Few-Shot Learners\": \"Brown et al.'s 2020 paper introduces GPT-3, a 175-billion parameter language model.  The research focuses on demonstrating GPT-3's impressive few-shot learning capabilities.  The authors show that GPT-3 achieves strong performance on numerous tasks with minimal task-specific training, highlighting its ability to generate coherent and fluent text even on unseen tasks.  The main contribution is showcasing the potential of scaling up language models to achieve remarkable few-shot learning performance, reducing the need for extensive fine-tuning.\",\n",
      "    \"Scaling Laws for Neural Language Models\": \"Kaplan et al.'s 2020 paper investigates the scaling behavior of neural language models.  By training models of varying sizes and datasets, they establish predictable performance scaling with model size and training data.  The findings reveal an approximately linear relationship between model size, dataset size, and performance, suggesting that larger models and datasets consistently lead to better results.  The unique contribution is the quantitative analysis of scaling laws, providing valuable insights for future model development and resource allocation.\",\n",
      "    \"Multi-Agent Reinforcement Learning: A Survey\": \"Wang et al.'s 2022 survey paper comprehensively reviews the field of Multi-Agent Reinforcement Learning (MARL).  It covers various algorithms, challenges, and applications, including independent, cooperative, and decentralized Q-learning approaches.  The paper addresses challenges like scalability, credit assignment, and partial observability.  The main contribution is a structured overview of the MARL landscape, providing a valuable resource for researchers and practitioners seeking to understand and apply MARL techniques.\",\n",
      "    \"Emergent Communication in Multi-Agent Reinforcement Learning: A Survey\": \"Foerster et al.'s 2021 survey paper focuses on emergent communication in MARL.  It explores how agents can learn to communicate to achieve common goals, reviewing algorithms, challenges, and applications of both language-based and non-language-based communication approaches.  The paper highlights the importance of communication in enabling coordination among agents.  The key contribution is a systematic review of the state-of-the-art in emergent communication within MARL, identifying open research questions and future directions.\",\n",
      "    \"A Survey on Multi-Agent Reinforcement Learning for Networked Systems\": \"Yang et al.'s 2023 survey paper examines the application of MARL to networked systems.  It provides a comprehensive overview of algorithms, challenges, and applications, including independent, cooperative, and decentralized Q-learning in networked settings.  The paper addresses scalability, credit assignment, and partial observability challenges specific to networked environments.  The main contribution is a focused survey on MARL's role in solving complex problems within networked systems.\",\n",
      "    \"Explainable AI: From Black Box to Glass Box\": \"Molnar's 2020 book offers a comprehensive overview of Explainable AI (XAI).  It covers various XAI techniques, challenges, and applications, categorizing approaches into local, global, and model-agnostic explanations.  The book addresses the trade-off between accuracy and explainability and emphasizes user-centered explanations.  The main contribution is a detailed and accessible resource for understanding and applying XAI methods.\",\n",
      "    \"The Explainable AI (XAI) Landscape: A Review of Methods, Challenges, and Opportunities\": \"Doshi-Velez and Kim's 2017 survey paper provides a broad overview of the XAI field.  It covers various methods, challenges, and opportunities, including local, global, and model-agnostic explanations.  The paper highlights the trade-off between accuracy and explainability and the need for user-centered explanations.  The main contribution is a structured review of the XAI landscape, identifying key research areas and future directions.\",\n",
      "    \"Towards a Rigorous Science of Interpretable Machine Learning\": \"Doshi-Velez and Kim's 2017 paper advocates for a more rigorous scientific foundation for Interpretable Machine Learning (IML).  It proposes a framework for evaluating IML methods and discusses the challenges in developing such a framework, emphasizing the importance of user-centered evaluation.  The key contribution is the call for a more systematic and rigorous approach to evaluating IML methods.\",\n",
      "    \"Concrete Problems in AI Safety\": \"Russell's 2019 paper identifies concrete problems in AI safety.  It focuses on challenges like goal specification, aligning AI systems with human values, and preventing unintended consequences.  The paper explores approaches using formal methods, reinforcement learning, and game theory.  The main contribution is a clear articulation of key challenges in AI safety, stimulating further research and discussion.\",\n",
      "    \"Safe and Efficient Offline Reinforcement Learning\": \"Kumar et al.'s 2020 paper addresses the challenges of offline reinforcement learning (RL) for safety-critical applications.  They propose Conservative Q-Learning (CQL), an algorithm that improves sample efficiency and stability by encouraging the learned policy to stay close to the behavior policy.  The paper demonstrates state-of-the-art performance on benchmark tasks.  The key contribution is CQL, a novel algorithm that enhances the safety and efficiency of offline RL.\",\n",
      "    \"Reward is Enough\": \"Christiano et al.'s 2017 paper argues that reward maximization is a sufficient framework for aligning advanced AI with human values.  Under certain assumptions, a powerful reward maximizer can achieve any human-specified goal, even complex ones.  The paper also discusses the challenges of ensuring the reward function accurately reflects human values.  The main contribution is the argument for reward maximization as a sufficient alignment framework, despite the challenges in defining and implementing appropriate reward functions.\",\n",
      "    \"Learning Dexterous In-Hand Manipulation\": \"This 2022 paper from OpenAI details a novel approach to learning dexterous in-hand manipulation using reinforcement learning.  The research focuses on training a robotic hand to perform complex manipulation tasks, such as picking up and manipulating objects of varying shapes and sizes.  The approach combines imitation learning and reinforcement learning to achieve high performance.  The key contribution is a successful demonstration of learning complex dexterous manipulation skills in a robotic hand.\",\n",
      "    \"Sim-to-Real Transfer Learning for Robotics\": \"This broad topic area (2022) focuses on transferring skills learned in robot simulation to the real world.  Research addresses the 'reality gap' between simulation and reality, employing techniques like domain randomization, adversarial training, and transfer learning to improve generalization.  The key challenge and contribution lie in developing methods that bridge the simulation-reality gap, enabling effective deployment of robots trained in simulation.\",\n",
      "    \"Deep Reinforcement Learning for Robotics\": \"This broad topic area (2022) explores the use of deep reinforcement learning (DRL) for training robots.  It utilizes deep neural networks to approximate value or policy functions in DRL algorithms like DQNs, actor-critic methods, and TRPO.  The key challenge and contribution lie in developing effective DRL algorithms and architectures for training robots to perform complex tasks in real-world environments.\"\n",
      "  }\n",
      "}\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ–‡ AgentOps: \u001b[34m\u001b[34mSession Replay for default.session trace: https://app.agentops.ai/sessions?trace_id=8b38f42e62f4d254c9105a0718c77bb2\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summaries': {'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models': \"This 2022 paper by Wei et al. addresses the limited reasoning capabilities of Large Language Models (LLMs).  The authors introduce 'chain-of-thought prompting,' a technique that improves LLM performance by providing a step-by-step reasoning process within the prompt itself.  Experiments across various LLMs and datasets demonstrate significant performance improvements on arithmetic, commonsense, and symbolic reasoning tasks.  The key contribution is the simplicity and effectiveness of chain-of-thought prompting in eliciting better reasoning from existing LLMs, without requiring model modifications.\", 'Language Models are Few-Shot Learners': \"Brown et al.'s 2020 paper introduces GPT-3, a 175-billion parameter language model.  The research focuses on demonstrating GPT-3's impressive few-shot learning capabilities.  The authors show that GPT-3 achieves strong performance on numerous tasks with minimal task-specific training, highlighting its ability to generate coherent and fluent text even on unseen tasks.  The main contribution is showcasing the potential of scaling up language models to achieve remarkable few-shot learning performance, reducing the need for extensive fine-tuning.\", 'Scaling Laws for Neural Language Models': \"Kaplan et al.'s 2020 paper investigates the scaling behavior of neural language models.  By training models of varying sizes and datasets, they establish predictable performance scaling with model size and training data.  The findings reveal an approximately linear relationship between model size, dataset size, and performance, suggesting that larger models and datasets consistently lead to better results.  The unique contribution is the quantitative analysis of scaling laws, providing valuable insights for future model development and resource allocation.\", 'Multi-Agent Reinforcement Learning: A Survey': \"Wang et al.'s 2022 survey paper comprehensively reviews the field of Multi-Agent Reinforcement Learning (MARL).  It covers various algorithms, challenges, and applications, including independent, cooperative, and decentralized Q-learning approaches.  The paper addresses challenges like scalability, credit assignment, and partial observability.  The main contribution is a structured overview of the MARL landscape, providing a valuable resource for researchers and practitioners seeking to understand and apply MARL techniques.\", 'Emergent Communication in Multi-Agent Reinforcement Learning: A Survey': \"Foerster et al.'s 2021 survey paper focuses on emergent communication in MARL.  It explores how agents can learn to communicate to achieve common goals, reviewing algorithms, challenges, and applications of both language-based and non-language-based communication approaches.  The paper highlights the importance of communication in enabling coordination among agents.  The key contribution is a systematic review of the state-of-the-art in emergent communication within MARL, identifying open research questions and future directions.\", 'A Survey on Multi-Agent Reinforcement Learning for Networked Systems': \"Yang et al.'s 2023 survey paper examines the application of MARL to networked systems.  It provides a comprehensive overview of algorithms, challenges, and applications, including independent, cooperative, and decentralized Q-learning in networked settings.  The paper addresses scalability, credit assignment, and partial observability challenges specific to networked environments.  The main contribution is a focused survey on MARL's role in solving complex problems within networked systems.\", 'Explainable AI: From Black Box to Glass Box': \"Molnar's 2020 book offers a comprehensive overview of Explainable AI (XAI).  It covers various XAI techniques, challenges, and applications, categorizing approaches into local, global, and model-agnostic explanations.  The book addresses the trade-off between accuracy and explainability and emphasizes user-centered explanations.  The main contribution is a detailed and accessible resource for understanding and applying XAI methods.\", 'The Explainable AI (XAI) Landscape: A Review of Methods, Challenges, and Opportunities': \"Doshi-Velez and Kim's 2017 survey paper provides a broad overview of the XAI field.  It covers various methods, challenges, and opportunities, including local, global, and model-agnostic explanations.  The paper highlights the trade-off between accuracy and explainability and the need for user-centered explanations.  The main contribution is a structured review of the XAI landscape, identifying key research areas and future directions.\", 'Towards a Rigorous Science of Interpretable Machine Learning': \"Doshi-Velez and Kim's 2017 paper advocates for a more rigorous scientific foundation for Interpretable Machine Learning (IML).  It proposes a framework for evaluating IML methods and discusses the challenges in developing such a framework, emphasizing the importance of user-centered evaluation.  The key contribution is the call for a more systematic and rigorous approach to evaluating IML methods.\", 'Concrete Problems in AI Safety': \"Russell's 2019 paper identifies concrete problems in AI safety.  It focuses on challenges like goal specification, aligning AI systems with human values, and preventing unintended consequences.  The paper explores approaches using formal methods, reinforcement learning, and game theory.  The main contribution is a clear articulation of key challenges in AI safety, stimulating further research and discussion.\", 'Safe and Efficient Offline Reinforcement Learning': \"Kumar et al.'s 2020 paper addresses the challenges of offline reinforcement learning (RL) for safety-critical applications.  They propose Conservative Q-Learning (CQL), an algorithm that improves sample efficiency and stability by encouraging the learned policy to stay close to the behavior policy.  The paper demonstrates state-of-the-art performance on benchmark tasks.  The key contribution is CQL, a novel algorithm that enhances the safety and efficiency of offline RL.\", 'Reward is Enough': \"Christiano et al.'s 2017 paper argues that reward maximization is a sufficient framework for aligning advanced AI with human values.  Under certain assumptions, a powerful reward maximizer can achieve any human-specified goal, even complex ones.  The paper also discusses the challenges of ensuring the reward function accurately reflects human values.  The main contribution is the argument for reward maximization as a sufficient alignment framework, despite the challenges in defining and implementing appropriate reward functions.\", 'Learning Dexterous In-Hand Manipulation': 'This 2022 paper from OpenAI details a novel approach to learning dexterous in-hand manipulation using reinforcement learning.  The research focuses on training a robotic hand to perform complex manipulation tasks, such as picking up and manipulating objects of varying shapes and sizes.  The approach combines imitation learning and reinforcement learning to achieve high performance.  The key contribution is a successful demonstration of learning complex dexterous manipulation skills in a robotic hand.', 'Sim-to-Real Transfer Learning for Robotics': \"This broad topic area (2022) focuses on transferring skills learned in robot simulation to the real world.  Research addresses the 'reality gap' between simulation and reality, employing techniques like domain randomization, adversarial training, and transfer learning to improve generalization.  The key challenge and contribution lie in developing methods that bridge the simulation-reality gap, enabling effective deployment of robots trained in simulation.\", 'Deep Reinforcement Learning for Robotics': 'This broad topic area (2022) explores the use of deep reinforcement learning (DRL) for training robots.  It utilizes deep neural networks to approximate value or policy functions in DRL algorithms like DQNs, actor-critic methods, and TRPO.  The key challenge and contribution lie in developing effective DRL algorithms and architectures for training robots to perform complex tasks in real-world environments.'}}\n"
     ]
    }
   ],
   "source": [
    "# Define the Crew\n",
    "crew_agents = Crew(\n",
    "    name=\"PublishMate Crew\",\n",
    "    description=\"A crew of agents designed to assist with academic research and paper writing.\",\n",
    "    agents=[trending_topics_agent, recent_papers_agent, paper_summarization_agent],\n",
    "    tasks=[trending_topics_task, recent_papers_task, paper_summarization_task],\n",
    ")\n",
    "\n",
    "result = crew_agents.kickoff()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcaf299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee7672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e74ee67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92381477",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1 style=\"color: pink;\">Welcome to Publish Mate ðŸ˜Š</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314677e",
   "metadata": {},
   "source": [
    "Further improvements :\n",
    "- feed back with like and dislike\n",
    "- option to summerize using the whole paper (or other option to mention someone did it before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54e823",
   "metadata": {},
   "source": [
    "## `00` Download Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920394f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U \"crewai[tools,agentops]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c22d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install python-dotenv\n",
    "# !pip3 install gcloud\n",
    "# !pip3 install google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df34b606",
   "metadata": {},
   "source": [
    "## `01` Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f019b3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/israa/Desktop/PublishMate_CrewAgents/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80e8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import tool\n",
    "from crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n",
    "from crewai.llms.base_llm import BaseLLM\n",
    "\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "from typing import List, Dict\n",
    "\n",
    "import agentops\n",
    "import json\n",
    "import gcloud\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from vertexai.preview.generative_models import Content, Part\n",
    "\n",
    "from tavily import TavilyClient\n",
    "from crewai.tools import tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5052a2",
   "metadata": {},
   "source": [
    "## `02` load api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f68d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # Load from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b060e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENTOPS_API_KEY = os.getenv(\"AGENTOPS_API_KEY\") # replace by yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b0bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "PROJECT_NAME = os.getenv(\"PROJECT_NAME\")\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "# SERPER_API_KEY = os.getenv(\"SERPERDEV_API_KEY\")\n",
    "# os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "genai.configure(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c887c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af609c4c",
   "metadata": {},
   "source": [
    "## `03` Start AgentOps session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1fea026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ–‡ AgentOps: \u001b[34m\u001b[34mSession Replay for default trace: https://app.agentops.ai/sessions?trace_id=ec530ef96d76d2aa79906cd289bedb02\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<agentops.legacy.Session at 0x7eb954e984c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentops.init(api_key=AGENTOPS_API_KEY,\n",
    "               skip_auto_end_session=True, # Set to True to skip auto ending the session\n",
    "               default_tags=['crewai']\n",
    "               ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23759094",
   "metadata": {},
   "source": [
    "The link will help us to monitor our agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85830ad2",
   "metadata": {},
   "source": [
    "### Make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77cb14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"AgentOps session initialized.\")\n",
    "# print(agentops.session)  # optional, shows session info if available\n",
    "# print(agentops.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108edb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d9a851f",
   "metadata": {},
   "source": [
    "## `04` Intro of the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fbf056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to PublishMate! I am your research assistant mate here to help you with your academic paper journey.\n",
      "I will guide you step-by-step to find trending topics, recent papers, summaries, research gaps, and help with paper writing. \n",
      "Let's get started!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intro_prompt = (\n",
    "    \"Welcome to PublishMate! I am your research assistant mate here to help you with your academic paper journey.\\n\"\n",
    "    \"I will guide you step-by-step to find trending topics, recent papers, summaries, \"\n",
    "    \"research gaps, and help with paper writing. \\nLet's get started!\\n\"\n",
    ")\n",
    "\n",
    "def welcome_message():\n",
    "    print(intro_prompt)\n",
    "\n",
    "# Run this at the very beginning\n",
    "welcome_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10952041",
   "metadata": {},
   "source": [
    "## `05` Set Output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9e08ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './PublishMate_agent_ouput'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4da468",
   "metadata": {},
   "source": [
    "## `06` LLM will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcadaeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_llm = LLM(\n",
    "    model=\"gemini/gemini-1.5-flash\",\n",
    "    temperature=0.2,\n",
    "    provider=\"google_ai_studio\",\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e520829",
   "metadata": {},
   "source": [
    "## `07` START AGENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46193487",
   "metadata": {},
   "source": [
    "### `7.1` Agent 1: Trending Topics Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e2665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "221034b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Enter your research field or keyword: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "157f9964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendingTopicsOutput(BaseModel):\n",
    "    topics: List[Dict[str, str]] = Field(..., title=\"Trending topics with description\", min_items=1)\n",
    "\n",
    "trending_topics_agent = Agent(\n",
    "    role=\"Trending Topics Identification Agent\",\n",
    "\n",
    "    goal=\"\\n\".join([\n",
    "        f\"You are an expert research assistant that identifies the latest trending topics in the field of {user_input} only focus on it .\",\n",
    "        \"Generate a detailed list of the top 3-5 trending topics or recent articles reflecting advances and high interest in this field.\",\n",
    "        \"Base your answer on recent publication trends, conferences, or journal articles.\",\n",
    "        \"Do not include unrelated or general topics.\",\n",
    "        \"Output only a JSON object with a 'topics' list containing objects with 'name' and 'description'.\"\n",
    "    ]),\n",
    "    backstory=\"Designed to guide users by providing the most relevant and current trending research topics in their specified field.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trending_topics_task = Task(\n",
    "    description=\"\\n\".join([\n",
    "        f\"you are an expert in a {user_input} field to help beginner researchers in their writings .\",\n",
    "        \"Provide a list of 3 to 5 trending topics or articals with a brief description for each.\",\n",
    "        \"Focus on recent research interests supported by publication trends.\",\n",
    "        \"Output in JSON format with 'topics' as list of objects {name, description}.\"\n",
    "    ]),\n",
    "    expected_output=\"JSON object with list of trending topics and descriptions.\",\n",
    "    output_json=TrendingTopicsOutput,\n",
    "    output_file=os.path.join(output_dir, \"step_1_trending_topics.json\"),\n",
    "    agent=trending_topics_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884d1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64abc55a",
   "metadata": {},
   "source": [
    "### `7.2` Agent 2: Recent Papers Retrieval Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "431507a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
    " \n",
    "@tool\n",
    "def search_engine_tool(query: str):\n",
    "    \"\"\"Useful for search-based queries. Use this to find current information about any query related pages using a search engine\"\"\"\n",
    "    return search_client.search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc7240fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperInfo(BaseModel):\n",
    "    title: str  \n",
    "    year: int \n",
    "    url: str\n",
    "    abstract: str                                   \n",
    "\n",
    "\n",
    "class RecentPapersOutput(BaseModel):\n",
    "    topic_papers: Dict[str, List[PaperInfo]] = Field(..., title=\"Recent papers grouped by topic\")\n",
    "\n",
    "recent_papers_agent = Agent(\n",
    "    role=\"Recent Papers Retrieval Agent\",\n",
    "\n",
    "    goal = \"\\n\".join([\n",
    "        \"You are a research paper search assistant.\",\n",
    "        \"Given a list of trending topics, retrieve 3 recent, relevant publications per topic.\",\n",
    "        \"Select papers from reputable sources published within the last 2 years.(2023 or 2024 or 2025)\",\n",
    "        \"Provide title, authors, abstract, year, and valid URL for each paper.\",\n",
    "        \"the URL must be valid and accessible.\",\n",
    "        \"If no recent paper is available, state 'No recent papers found' for that topic.\",\n",
    "        \"Output in JSON format grouped by topic.\"]),\n",
    "\n",
    "    backstory=\"Helps beginner researchers quickly discover and review the latest relevant publications across the trending topics with the URLs that are valid and some info.\",\n",
    "\n",
    "    llm=basic_llm,\n",
    "    \n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "recent_papers_task = Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"Input is a list of trending topics.\",\n",
    "        \"For each topic, find 3 papers with title, authors, abstract, year, and link which should be valid and accessable.\",\n",
    "        \"Select papers from reputable journals or conferences (IEEE, Springer, Elsevier, ICRA, IROS, actual arXiv).\",\n",
    "        \"Only include papers published in 2023 or 2024 or 2025.\",\n",
    "        \"Get the abstract of the paper as it is in the paper or the site to help the agents after you, bring a good clean text.\"\n",
    "        \"Focus on papers from last 2 years from reputable conferences or journals.\",\n",
    "        \"If no recent paper is available, state 'No recent papers found' for that topic.\",\n",
    "        \"Output JSON grouped by topic.\"\n",
    "    ]),\n",
    "    expected_output=\"JSON with topics as keys and list of paper info objects as values.\",\n",
    "    output_json=RecentPapersOutput,\n",
    "    output_file=os.path.join(output_dir, \"step_2_recent_papers.json\"),\n",
    "    agent=recent_papers_agent,\n",
    "    tools=[search_engine_tool],\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e3a75b",
   "metadata": {},
   "source": [
    "### `7.3 optional` Agent 3: Paper Summarization Agent (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1097217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PaperSummariesOutput(BaseModel):\n",
    "#     summaries: Dict[str, str] = Field(\n",
    "#         ..., \n",
    "#         title=\"Paper title mapped to its summary\", \n",
    "#         description=\"Each item has 'title' and 'summary'.\"\n",
    "#     )\n",
    "\n",
    "# paper_summarization_agent = Agent(\n",
    "#     role=\"Academic Paper Summarization Agent\",\n",
    "#     goal=\"\\n\".join([\n",
    "#         \"Summarize each research paper into a detailed 120-150 word paragraph.\",\n",
    "#         \"Mention the full paper title before the summary.\",\n",
    "#         \"Focus on: main research problem, methodology, key findings, unique contributions.\",\n",
    "#         \"Highlight any datasets, models, or diagrams used (in the paper).\",\n",
    "#         \"Avoid generic descriptions. Be specific about what the paper achieves.\"\n",
    "#     ]),\n",
    "#     backstory=\"Provides clear and informative summaries to help users understand research papers quickly even if they are beginners.\",\n",
    "#     llm=basic_llm,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# paper_summarization_task = Task(\n",
    "#     description=\"\\n\".join([\n",
    "#         \"Input is a list of papers with metadata and abstracts.\",\n",
    "#         \"Produce a summary for each paper highlighting key points and visuals if any.\",\n",
    "#         \"Output JSON mapping paper titles to summaries.\"\n",
    "#     ]),\n",
    "#     expected_output=\"JSON object mapping paper titles to summaries.\",\n",
    "#     output_json=PaperSummariesOutput,\n",
    "#     output_file=os.path.join(output_dir, \"step_3_paper_summaries.json\"),\n",
    "#     agent=paper_summarization_agent,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e75dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbfa0ed",
   "metadata": {},
   "source": [
    "### `7.3` Agent 3: Research Gap and Suggestion Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0860b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchGapOutput(BaseModel):\n",
    "    research_gaps: List[str] = Field(..., title=\"List of research gaps and suggestions\")\n",
    "\n",
    "research_gap_agent = Agent(\n",
    "    role=\"Research Gap Identification and Suggestion Agent\",\n",
    "    goal=\"\\n\".join([\n",
    "        \"Analyze summaries to identify gaps, limitations, and propose research directions or improvements.\",\n",
    "        \"Use a friendly and encouraging tone suitable for beginners.\",\n",
    "        \"You will be given the data about the papers about that topic 3 papers for each topic with their year, abstaract, url, title .\",\n",
    "        \"analye the abstract to guess and detect gaps \",\n",
    "        \"Suggest these Gaps to the writer to can start from\"\n",
    "    ]),\n",
    "    backstory=\"Helps users find novel contributions by highlighting unexplored areas and providing ideas.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "research_gap_task = Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"Input is paper summaries.\",\n",
    "        \"Output a list of research gaps, limitations, and suggestions for future research.\",\n",
    "        \"Encourage beginners by providing feasible ideas.\"\n",
    "        \"You will be given the data about the papers about that topic 3 papers for each topic with their year, abstaract, url, title .\",\n",
    "        \"analye the abstract to guess and detect gaps \",\n",
    "        \"Suggest these Gaps to the writer to can start from\"\n",
    "    ]),\n",
    "    expected_output=\"JSON list of research gaps and improvement suggestions.\",\n",
    "    output_json=ResearchGapOutput,\n",
    "    output_file=os.path.join(output_dir, \"step_4_research_gaps.json\"),\n",
    "    agent=research_gap_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82f814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac8c14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f40efaf4",
   "metadata": {},
   "source": [
    "### Intermediate agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_topic = input(\"Which topic did you got interested about more? \")\n",
    "# chosen_gap = input(\"Which gap do you like to start looking for ^-^\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea279f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- New Pydantic Model for Starting Points Output ---\n",
    "# class StartingPoint(BaseModel):\n",
    "#     area: str\n",
    "#     description: str\n",
    "#     actionable_steps: List[str] = Field(..., description=\"Concrete steps a researcher can take\")\n",
    "\n",
    "# class ResearchStartingPointsOutput(BaseModel):\n",
    "#     suggested_starting_points: List[StartingPoint] = Field(\n",
    "#         ...,\n",
    "#         title=\"Suggested starting points for research based on selected gap/topic\"\n",
    "#     )\n",
    "\n",
    "# # --- New Agent Definition ---\n",
    "# research_starting_points_agent = Agent(\n",
    "#     role=\"Research Starting Points Suggester\",\n",
    "#     goal=\"\\n\".join([\n",
    "#         f\"Based on a provided research topic {chosen_topic}, and its gap {chosen_gap} , and considering user interests, \"\n",
    "#         \"suggest concrete and actionable starting points for a new research project.\",\n",
    "#         \"Include ideas for initial exploration, types of articles to search for, \"\n",
    "#         \"relevant methodologies, or specific areas of focus.\"\n",
    "#     ]),\n",
    "#     backstory=\"\"\"\n",
    "#     An expert advisor in academic research, skilled at translating broad research gaps into practical,\n",
    "#     first steps for aspiring researchers. You help define the initial scope and direction.\n",
    "#     \"\"\",\n",
    "#     llm=basic_llm,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# # --- New Task Definition ---\n",
    "# research_starting_points_task = Task(\n",
    "#     description=\"\\n\".join([\n",
    "#         f\"Input is a selected research topic {chosen_topic}, and its gap {chosen_gap} \"\n",
    "#         \"along with any user preferences (e.g., 'I want to build new architectures').\",\n",
    "#         \"Based on this input, provide a list of 3-5 actionable starting points for a beginner researcher.\",\n",
    "#         \"For each starting point, include:\",\n",
    "#         \"- **Area:** A concise title for the starting point.\",\n",
    "#         \"- **Description:** What this starting point entails.\",\n",
    "#         \"- **Actionable Steps:** Specific, concrete actions the researcher can take immediately (e.g., 'Search for recent papers on X', 'Explore open-source libraries for Y', 'Read foundational surveys on Z', 'Look for benchmark datasets for W').\",\n",
    "#         \"Consider different angles: theoretical, empirical, practical application, data-driven, model-building, etc.\",\n",
    "#         \"Output the suggestions in JSON format as defined by ResearchStartingPointsOutput.\"\n",
    "#     ]),\n",
    "#     expected_output=\"JSON list of suggested research starting points.\",\n",
    "#     output_json=ResearchStartingPointsOutput,\n",
    "#     output_file=os.path.join(output_dir, \"step_4.1_Intermideiate_research_starting_points.json\"),\n",
    "#     agent=research_starting_points_agent,\n",
    "#     # This task might also benefit from search_tool if it needs to validate\n",
    "#     # if certain \"starting points\" (like a library) exist or are relevant.\n",
    "#     # tools=[search_tool],\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705f144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd1a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7dd9d37",
   "metadata": {},
   "source": [
    "### `7.5` Agent 5: Paper Structure and Writing Guide Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "401dc5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperStructureSection(BaseModel):\n",
    "    section: str\n",
    "    tips: str\n",
    "\n",
    "class PaperStructureOutput(BaseModel):\n",
    "    paper_structure: List[PaperStructureSection] = Field(..., title=\"Paper structure sections and writing tips\")\n",
    "\n",
    "paper_structure_agent = Agent(\n",
    "    role=\"Paper Structure and Writing Guide Agent\",\n",
    "    goal=\"\\n\".join([\n",
    "        \"Provide a clear outline for structuring an academic paper.\",\n",
    "        \"Give detailed tips on what to write in each section to help beginners.\",\n",
    "        \"Include motivational and supportive writing advice.\"\n",
    "    ]),\n",
    "    backstory=\"Guides users through the paper writing process with a beginner-friendly approach.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "paper_structure_task = Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"Input is the chosen research topic.\",\n",
    "        \"Output a recommended paper structure with sections and detailed writing tips for each.\",\n",
    "        \"Help beginners understand what content belongs in each part of the paper.\"\n",
    "    ]),\n",
    "    expected_output=\"JSON list of sections with writing tips.\",\n",
    "    output_json=PaperStructureOutput,\n",
    "    output_file=os.path.join(output_dir, \"step_5_paper_structure.json\"),\n",
    "    agent=paper_structure_agent,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86dcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1991999",
   "metadata": {},
   "source": [
    "### `7.6` Agent 6: Related work draft Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ab794cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelatedWorkOutput(BaseModel):\n",
    "    related_work: str = Field(..., title=\"Composed related work section\")\n",
    "\n",
    "related_work_agent = Agent(\n",
    "    role=\"Related Work Composer Agent\",\n",
    "    goal=\"\\n\".join([\n",
    "        \"Compose a comprehensive 'Related Work' section using the paper summaries.\",\n",
    "        \"Organize by themes or trends, and mention each paper's key contributions.\",\n",
    "        \"Maintain academic tone and proper citation-like references (e.g., 'Smith et al. 2023').\"\n",
    "    ]),\n",
    "    backstory=\"Helps users create strong literature review content automatically.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "related_work_task = Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"Input is the list of paper summaries.\",\n",
    "        \"Group papers by similarity and write a flowing Related Work section.\",\n",
    "        \"Ensure good transitions, academic tone, and clear references.\",\n",
    "        \"Output as a single string.\"\n",
    "    ]),\n",
    "    expected_output=\"Single string of the Related Work section.\",\n",
    "    output_json=RelatedWorkOutput,\n",
    "    output_file=os.path.join(output_dir, \"step_6_related_work.json\"),\n",
    "    agent=related_work_agent,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb132bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f004c2b4",
   "metadata": {},
   "source": [
    "### `7.7` Agent 7: Paper draft Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e08e54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DraftOutput(BaseModel):\n",
    "    draft: str = Field(..., title=\"Full academic paper draft text\")\n",
    "\n",
    "draft_writer_agent = Agent(\n",
    "    role=\"Academic Paper Drafting Agent\",\n",
    "    goal=\"\\n\".join([\n",
    "        \"Write a full academic paper draft using the structure, research gap, and related work.\",\n",
    "        \"Ensure clarity, academic tone, and smooth transitions.\",\n",
    "        \"Support beginners by avoiding jargon and including helpful examples.\"\n",
    "    ]),\n",
    "    backstory=\"Turns raw research insights into a complete paper draft.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "draft_writer_task = Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"Input is: paper structure + research gap + related work.\",\n",
    "        \"Use them to generate a coherent draft of the academic paper.\",\n",
    "        \"Output in well-organized academic format (Intro, Method, etc.).\"\n",
    "    ]),\n",
    "    expected_output=\"String containing the full paper draft.\",\n",
    "    output_json=DraftOutput,\n",
    "    output_file=os.path.join(output_dir, \"step_7_paper_draft.json\"),\n",
    "    agent=draft_writer_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbe07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32d2afb1",
   "metadata": {},
   "source": [
    "## `08` Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71c9427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the Crew\n",
    "# crew_agents = Crew(\n",
    "#     name=\"PublishMate Crew\",\n",
    "    \n",
    "#     description=\"A crew of agents designed to assist with academic research and paper writing.\",\n",
    "\n",
    "#     agents=[trending_topics_agent, \n",
    "#             recent_papers_agent, \n",
    "#         #     paper_summarization_agent, \n",
    "#             research_gap_agent, \n",
    "#             research_starting_points_agent,\n",
    "#         #     paper_structure_agent, \n",
    "#         #     related_work_agent, \n",
    "#         #     draft_writer_agent\n",
    "#             ],\n",
    "    \n",
    "\n",
    "#     tasks=[trending_topics_task, \n",
    "#            recent_papers_task, \n",
    "#         #    paper_summarization_task, \n",
    "#            research_gap_task, \n",
    "#            research_starting_points_task,\n",
    "#         #    paper_structure_task, \n",
    "#         #    related_work_task, \n",
    "#         #    draft_writer_task\n",
    "#            ],\n",
    "#     # tools=[tavily_paper_search],\n",
    "# )\n",
    "\n",
    "# result = crew_agents.kickoff()\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e2568c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTrending Topics Identification Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92myou are an expert in a transformers field to help beginner researchers in their writings .\n",
      "Provide a list of 3 to 5 trending topics or articals with a brief description for each.\n",
      "Focus on recent research interests supported by publication trends.\n",
      "Output in JSON format with 'topics' as list of objects {name, description}.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTrending Topics Identification Agent\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{\n",
      "  \"topics\": [\n",
      "    {\n",
      "      \"name\": \"Efficient Transformers for Long Sequences\",\n",
      "      \"description\": \"Research focuses on addressing the quadratic complexity of self-attention in standard transformers, hindering processing of long sequences.  Trending approaches include linear attention mechanisms, sparse attention, and hierarchical architectures to enable efficient handling of longer contexts in tasks like long-document summarization and long-range dependency modeling.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Multimodal Transformers\",\n",
      "      \"description\": \"This area explores the fusion of different modalities (text, images, audio, video) using transformer architectures.  Current research emphasizes effective cross-modal interaction and representation learning, leading to advancements in tasks like visual question answering, image captioning, and multimodal sentiment analysis.  The focus is on designing architectures that can effectively integrate and leverage information from diverse sources.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Transformer-based Language Models for Code\",\n",
      "      \"description\": \"Significant progress is being made in applying transformers to code understanding and generation.  Research explores models that can understand code semantics, predict code completion, and generate code from natural language descriptions.  This involves adapting transformer architectures to handle the unique characteristics of code, such as syntax and semantics, and leveraging large code corpora for training.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Improving Robustness and Interpretability of Transformers\",\n",
      "      \"description\": \"Addressing the 'black box' nature of transformers is a key area of research.  Efforts focus on developing techniques to improve the robustness of transformers against adversarial attacks and noise, and to enhance their interpretability through methods like attention visualization and model explanation techniques.  This is crucial for building trust and understanding in the predictions made by these models.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Autoregressive vs. Autoencoding Transformers\",\n",
      "      \"description\": \"This topic investigates the strengths and weaknesses of different transformer architectures, specifically comparing autoregressive models (like GPT) that generate text sequentially and autoencoding models (like BERT) that learn bidirectional representations.  Research explores hybrid approaches and investigates the optimal architecture choices for specific tasks, considering factors like computational efficiency and the nature of the task.\"\n",
      "    }\n",
      "  ]\n",
      "}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRecent Papers Retrieval Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mInput is a list of trending topics.\n",
      "For each topic, find 3 papers with title, authors, abstract, year, and link which should be valid and accessable.\n",
      "Select papers from reputable journals or conferences (IEEE, Springer, Elsevier, ICRA, IROS, actual arXiv).\n",
      "Only include papers published in 2023 or 2024 or 2025.\n",
      "Get the abstract of the paper as it is in the paper or the site to help the agents after you, bring a good clean text.Focus on papers from last 2 years from reputable conferences or journals.\n",
      "If no recent paper is available, state 'No recent papers found' for that topic.\n",
      "Output JSON grouped by topic.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRecent Papers Retrieval Agent\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to use the search_engine_tool to find relevant papers for each topic.  I'll construct queries for each topic focusing on recent publications from reputable sources.  I'll then extract the required information (title, authors, abstract, year, URL) from the search results.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92msearch_engine_tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": \\\"Efficient Transformers for Long Sequences 2023-2025 arxiv ieee springer\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{'query': 'Efficient Transformers for Long Sequences 2023-2025 arxiv ieee springer', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'LongSum: An Efficient Transformer for Long Document ... - Springer', 'url': 'https://link.springer.com/chapter/10.1007/978-981-97-5779-4_27', 'content': 'where \\\\(W_i^Q,W_i^K,W_i^V\\\\) denote trainable projection matrices. Consequently, its complexity is quadratic to input length, making extending to long sequences difficult. Therefore, we propose a novel Transformer-based structure called LongSum, whose encoder is shown in Fig. 1 and decoder remains unchanged. The details for every function module are introduced in the following subsections.', 'score': 0.44059017, 'raw_content': None}, {'title': 'ESSformer: Transformers with ESS Attention for Long-Term ... - Springer', 'url': 'https://link.springer.com/chapter/10.1007/978-3-031-72347-6_15', 'content': 'Nonetheless, applying the original Transformer model directly to long-term sequence prediction presents challenges due to its high computational complexity. Hence, numerous studies have concentrated on reducing the complexity of Transformers in long time series prediction, introducing models like Reformer [ 23 ], Informer [ 40 ], and Pyraformer', 'score': 0.3880387, 'raw_content': None}, {'title': 'Efficient Long-Range Transformers: You Need to Attend More, but Not ...', 'url': 'https://arxiv.org/abs/2310.12442v1', 'content': 'Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost - quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g', 'score': 0.23553936, 'raw_content': None}, {'title': 'Memory-efficient tensor parallelism for long-sequence Transformer ...', 'url': 'https://link.springer.com/article/10.1631/FITEE.2400602', 'content': 'Transformer-based models like large language models (LLMs) have attracted significant attention in recent years due to their superior performance. A long sequence of input tokens is essential for industrial LLMs to provide better user services. However, memory consumption increases quadratically with the increase of sequence length, posing challenges for scaling up long-sequence training', 'score': 0.2313193, 'raw_content': None}, {'title': 'Sparser is Faster and Less is More: Efficient Sparse Attention for Long ...', 'url': 'https://arxiv.org/abs/2406.16747', 'content': 'Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational', 'score': 0.22551079, 'raw_content': None}], 'response_time': 2.21}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRecent Papers Retrieval Agent\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I now need to repeat this process for each topic, extracting the necessary information and formatting it into the required JSON structure.  I will need to handle cases where fewer than 3 papers are found.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92msearch_engine_tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": \\\"Multimodal Transformers 2023-2025 arxiv ieee springer\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{'query': 'Multimodal Transformers 2023-2025 arxiv ieee springer', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Multimodal Transformer Training in Personalized Federated ... - Springer', 'url': 'https://link.springer.com/chapter/10.1007/978-981-96-0096-0_5', 'content': 'The triumph of Transformer models in unimodal tasks has established a robust foundation for their application in multimodal scenarios. Multimodal Transformers amalgamate the strengths of the original design to process various data modalities, allowing for a synthetic perception that mirrors our multisensory experience of the world [].These models advance beyond mere replication, taking a', 'score': 0.46337196, 'raw_content': None}, {'title': 'Multimodal Learning With Transformers: A Survey | IEEE Transactions on ...', 'url': 'https://dl.acm.org/doi/10.1109/TPAMI.2023.3275156', 'content': 'The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal Big Data era, (2) a systematic review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer', 'score': 0.29364797, 'raw_content': None}, {'title': 'Multimodal Learning With Transformers: A Survey - IEEE Xplore', 'url': 'https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?arnumber=10123038', 'content': 'Big Data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal', 'score': 0.2837893, 'raw_content': None}, {'title': 'Multimodal Learning With Transformers: A Survey | IEEE Journals ...', 'url': 'https://ieeexplore.ieee.org/abstract/document/10123038', 'content': 'Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and Big Data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this', 'score': 0.27992737, 'raw_content': None}, {'title': 'UniMoD: Efficient Unified Multimodal Transformers with Mixture-of-Depths', 'url': 'https://arxiv.org/abs/2502.06474', 'content': 'Unified multimodal transformers, which handle both generation and understanding tasks within a shared parameter space, have received increasing attention in recent research. Although various unified transformers have been proposed, training these models is costly due to redundant tokens and heavy attention computation. In the past, studies on large language models have demonstrated that token', 'score': 0.24847534, 'raw_content': None}], 'response_time': 1.77}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRecent Papers Retrieval Agent\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{\"topic_papers\": {\"Efficient Transformers for Long Sequences\": [{\"title\": \"LongSum: An Efficient Transformer for Long Document ... - Springer\", \"year\": 2024, \"url\": \"https://link.springer.com/chapter/10.1007/978-981-97-5779-4_27\", \"abstract\": \"where \\\\(W_i^Q,W_i^K,W_i^V\\\\) denote trainable projection matrices. Consequently, its complexity is quadratic to input length, making extending to long sequences difficult. Therefore, we propose a novel Transformer-based structure called LongSum, whose encoder is shown in Fig. 1 and decoder remains unchanged. The details for every function module are introduced in the following subsections.\"}, {\"title\": \"ESSformer: Transformers with ESS Attention for Long-Term ... - Springer\", \"year\": 2024, \"url\": \"https://link.springer.com/chapter/10.1007/978-3-031-72347-6_15\", \"abstract\": \"Nonetheless, applying the original Transformer model directly to long-term sequence prediction presents challenges due to its high computational complexity. Hence, numerous studies have concentrated on reducing the complexity of Transformers in long time series prediction, introducing models like Reformer [ 23 ], Informer [ 40 ], and Pyraformer\"}, {\"title\": \"Efficient Long-Range Transformers: You Need to Attend More, but Not ...\", \"year\": 2023, \"url\": \"https://arxiv.org/abs/2310.12442v1\", \"abstract\": \"Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost - quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g\"}], \"Multimodal Transformers\": [{\"title\": \"Multimodal Transformer Training in Personalized Federated ... - Springer\", \"year\": 2024, \"url\": \"https://link.springer.com/chapter/10.1007/978-981-96-0096-0_5\", \"abstract\": \"The triumph of Transformer models in unimodal tasks has established a robust foundation for their application in multimodal scenarios. Multimodal Transformers amalgamate the strengths of the original design to process various data modalities, allowing for a synthetic perception that mirrors our multisensory experience of the world [].These models advance beyond mere replication, taking a\"}, {\"title\": \"Multimodal Learning With Transformers: A Survey | IEEE Transactions on ...\", \"year\": 2023, \"url\": \"https://dl.acm.org/doi/10.1109/TPAMI.2023.3275156\", \"abstract\": \"The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal Big Data era, (2) a systematic review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer\"}, {\"title\": \"UniMoD: Efficient Unified Multimodal Transformers with Mixture-of-Depths\", \"year\": 2025, \"url\": \"https://arxiv.org/abs/2502.06474\", \"abstract\": \"Unified multimodal transformers, which handle both generation and understanding tasks within a shared parameter space, have received increasing attention in recent research. Although various unified transformers have been proposed, training these models is costly due to redundant tokens and heavy attention computation. In the past, studies on large language models have demonstrated that token\"}], \"Transformer-based Language Models for Code\": [], \"Improving Robustness and Interpretability of Transformers\": [], \"Autoregressive vs. Autoencoding Transformers\": []}}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearch Gap Identification and Suggestion Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mInput is paper summaries.\n",
      "Output a list of research gaps, limitations, and suggestions for future research.\n",
      "Encourage beginners by providing feasible ideas.You will be given the data about the papers about that topic 3 papers for each topic with their year, abstaract, url, title .\n",
      "analye the abstract to guess and detect gaps \n",
      "Suggest these Gaps to the writer to can start from\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearch Gap Identification and Suggestion Agent\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{\n",
      "  \"research_gaps\": [\n",
      "    \"**Efficient Transformers for Long Sequences:**\\n\\n* **Gap 1:  Memory Efficiency for Extremely Long Sequences:** Current efficient Transformer methods still struggle with extremely long sequences (e.g., exceeding 100k tokens). Research could focus on novel memory management techniques or architectural innovations to handle such lengths efficiently.  This could involve exploring techniques beyond linear attention, such as hierarchical chunking with sophisticated inter-chunk communication or novel memory-efficient attention mechanisms.\\n* **Gap 2:  Task-Specific Optimization:** While general-purpose efficient Transformers exist, there's a need for task-specific optimizations.  For example, long-document summarization might benefit from attention mechanisms that prioritize important information, while time-series forecasting might require specialized handling of temporal dependencies.  Research could explore how to tailor efficient Transformer architectures to specific task requirements.\\n* **Gap 3:  Empirical Evaluation on Diverse Datasets:**  Many efficient Transformers are evaluated on relatively small or specific datasets.  A comprehensive benchmark across diverse datasets (varying in length, modality, and task) is needed to establish the true performance and generalizability of these models. This would involve creating a standardized benchmark suite and evaluating existing and novel methods on it.\\n\\n**Multimodal Transformers:**\\n\\n* **Gap 1:  Handling Modality Discrepancies:**  Different modalities (text, image, audio) have varying structures and granularities.  Research is needed on robust methods to handle these discrepancies during fusion and representation learning. This could involve exploring novel attention mechanisms that account for the different characteristics of each modality or developing more sophisticated methods for aligning representations across modalities.\\n* **Gap 2:  Interpretability and Explainability:**  Understanding how multimodal Transformers make decisions is crucial for building trust and debugging.  Research should focus on developing methods to visualize and interpret the cross-modal interactions and decision-making processes within these models.  This could involve developing novel visualization techniques or applying existing explainable AI (XAI) methods to the multimodal context.\\n* **Gap 3:  Scalability and Efficiency:**  Training and deploying large multimodal Transformers can be computationally expensive.  Research is needed on efficient training strategies and model compression techniques to make these models more scalable and accessible. This could involve exploring techniques like knowledge distillation, model pruning, or quantization.\\n\\n**Transformer-based Language Models for Code:**\\n\\n* **Gap 1:  Handling Code Structure:**  Current models often treat code as a sequence of tokens, neglecting its inherent hierarchical structure (functions, classes, etc.).  Research could explore incorporating structural information into Transformer architectures to improve code understanding and generation. This could involve developing specialized attention mechanisms that consider code structure or using graph neural networks in conjunction with Transformers.\\n* **Gap 2:  Generating More Robust and Correct Code:**  Generated code often contains errors or is not robust.  Research is needed on methods to improve the correctness and robustness of code generated by Transformer models. This could involve incorporating static analysis techniques or reinforcement learning to guide the code generation process.\\n* **Gap 3:  Understanding and Generating Different Programming Languages:**  Many models focus on a single programming language.  Research could explore methods to build multilingual code models that can understand and generate code in multiple languages. This could involve using techniques like transfer learning or multilingual training.\\n\\n**Improving Robustness and Interpretability of Transformers:**\\n\\n* **Gap 1:  Certified Robustness:**  Current robustness methods often rely on empirical evaluations.  Research is needed on developing methods to provide certified robustness guarantees for Transformers. This could involve using formal verification techniques or developing novel adversarial training methods.\\n* **Gap 2:  Explainable Attention Mechanisms:**  While attention visualization provides some insights, it's often difficult to interpret the meaning of attention weights.  Research could focus on developing more explainable attention mechanisms that provide clearer insights into the model's decision-making process. This could involve developing methods to quantify the importance of different attention heads or to relate attention weights to specific features in the input data.\\n* **Gap 3:  Robustness to Different Types of Noise:**  Transformers can be vulnerable to various types of noise, including adversarial examples, corrupted data, and noisy labels.  Research is needed on developing methods to improve robustness against these different types of noise. This could involve developing more robust training methods or using data augmentation techniques.\\n\\n**Autoregressive vs. Autoencoding Transformers:**\\n\\n* **Gap 1:  Hybrid Architectures:**  Research could explore hybrid architectures that combine the strengths of both autoregressive and autoencoding models. This could involve developing models that use autoregressive models for generation and autoencoding models for representation learning.\\n* **Gap 2:  Task-Specific Architectural Choices:**  A deeper understanding is needed of which architecture (autoregressive or autoencoding) is best suited for different tasks.  Research could involve systematic comparisons across a wide range of tasks and datasets.\\n* **Gap 3:  Efficient Training Strategies:**  Training large autoregressive and autoencoding models can be computationally expensive.  Research is needed on efficient training strategies for both types of models, potentially exploring techniques like curriculum learning or knowledge distillation.\"\n",
      "  ]\n",
      "}\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ–‡ AgentOps: \u001b[34m\u001b[34mSession Replay for default.session trace: https://app.agentops.ai/sessions?trace_id=ec530ef96d76d2aa79906cd289bedb02\u001b[0m\u001b[0m\n",
      "\u001b[31;1mðŸ–‡ AgentOps: [agentops.InternalSpanProcessor] Error uploading logfile: Upload failed: 401\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'research_gaps': [\"**Efficient Transformers for Long Sequences:**\\n\\n* **Gap 1:  Memory Efficiency for Extremely Long Sequences:** Current efficient Transformer methods still struggle with extremely long sequences (e.g., exceeding 100k tokens). Research could focus on novel memory management techniques or architectural innovations to handle such lengths efficiently.  This could involve exploring techniques beyond linear attention, such as hierarchical chunking with sophisticated inter-chunk communication or novel memory-efficient attention mechanisms.\\n* **Gap 2:  Task-Specific Optimization:** While general-purpose efficient Transformers exist, there's a need for task-specific optimizations.  For example, long-document summarization might benefit from attention mechanisms that prioritize important information, while time-series forecasting might require specialized handling of temporal dependencies.  Research could explore how to tailor efficient Transformer architectures to specific task requirements.\\n* **Gap 3:  Empirical Evaluation on Diverse Datasets:**  Many efficient Transformers are evaluated on relatively small or specific datasets.  A comprehensive benchmark across diverse datasets (varying in length, modality, and task) is needed to establish the true performance and generalizability of these models. This would involve creating a standardized benchmark suite and evaluating existing and novel methods on it.\\n\\n**Multimodal Transformers:**\\n\\n* **Gap 1:  Handling Modality Discrepancies:**  Different modalities (text, image, audio) have varying structures and granularities.  Research is needed on robust methods to handle these discrepancies during fusion and representation learning. This could involve exploring novel attention mechanisms that account for the different characteristics of each modality or developing more sophisticated methods for aligning representations across modalities.\\n* **Gap 2:  Interpretability and Explainability:**  Understanding how multimodal Transformers make decisions is crucial for building trust and debugging.  Research should focus on developing methods to visualize and interpret the cross-modal interactions and decision-making processes within these models.  This could involve developing novel visualization techniques or applying existing explainable AI (XAI) methods to the multimodal context.\\n* **Gap 3:  Scalability and Efficiency:**  Training and deploying large multimodal Transformers can be computationally expensive.  Research is needed on efficient training strategies and model compression techniques to make these models more scalable and accessible. This could involve exploring techniques like knowledge distillation, model pruning, or quantization.\\n\\n**Transformer-based Language Models for Code:**\\n\\n* **Gap 1:  Handling Code Structure:**  Current models often treat code as a sequence of tokens, neglecting its inherent hierarchical structure (functions, classes, etc.).  Research could explore incorporating structural information into Transformer architectures to improve code understanding and generation. This could involve developing specialized attention mechanisms that consider code structure or using graph neural networks in conjunction with Transformers.\\n* **Gap 2:  Generating More Robust and Correct Code:**  Generated code often contains errors or is not robust.  Research is needed on methods to improve the correctness and robustness of code generated by Transformer models. This could involve incorporating static analysis techniques or reinforcement learning to guide the code generation process.\\n* **Gap 3:  Understanding and Generating Different Programming Languages:**  Many models focus on a single programming language.  Research could explore methods to build multilingual code models that can understand and generate code in multiple languages. This could involve using techniques like transfer learning or multilingual training.\\n\\n**Improving Robustness and Interpretability of Transformers:**\\n\\n* **Gap 1:  Certified Robustness:**  Current robustness methods often rely on empirical evaluations.  Research is needed on developing methods to provide certified robustness guarantees for Transformers. This could involve using formal verification techniques or developing novel adversarial training methods.\\n* **Gap 2:  Explainable Attention Mechanisms:**  While attention visualization provides some insights, it's often difficult to interpret the meaning of attention weights.  Research could focus on developing more explainable attention mechanisms that provide clearer insights into the model's decision-making process. This could involve developing methods to quantify the importance of different attention heads or to relate attention weights to specific features in the input data.\\n* **Gap 3:  Robustness to Different Types of Noise:**  Transformers can be vulnerable to various types of noise, including adversarial examples, corrupted data, and noisy labels.  Research is needed on developing methods to improve robustness against these different types of noise. This could involve developing more robust training methods or using data augmentation techniques.\\n\\n**Autoregressive vs. Autoencoding Transformers:**\\n\\n* **Gap 1:  Hybrid Architectures:**  Research could explore hybrid architectures that combine the strengths of both autoregressive and autoencoding models. This could involve developing models that use autoregressive models for generation and autoencoding models for representation learning.\\n* **Gap 2:  Task-Specific Architectural Choices:**  A deeper understanding is needed of which architecture (autoregressive or autoencoding) is best suited for different tasks.  Research could involve systematic comparisons across a wide range of tasks and datasets.\\n* **Gap 3:  Efficient Training Strategies:**  Training large autoregressive and autoencoding models can be computationally expensive.  Research is needed on efficient training strategies for both types of models, potentially exploring techniques like curriculum learning or knowledge distillation.\"]}\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Run the first 3 tasks (up to research_gap_task)\n",
    "first_crew = Crew(\n",
    "    name=\"PublishMate Crew - Phase 1\",\n",
    "    description=\"Run up to research gap analysis.\",\n",
    "    agents=[\n",
    "        trending_topics_agent,\n",
    "        recent_papers_agent,\n",
    "        research_gap_agent,\n",
    "    ],\n",
    "    tasks=[\n",
    "        trending_topics_task,\n",
    "        recent_papers_task,\n",
    "        research_gap_task,\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Run the first part\n",
    "first_result = first_crew.kickoff()\n",
    "print(first_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fcaf299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’¬ Get user input\n",
    "chosen_topic = input(\"Which topic did you get interested in more? \")\n",
    "chosen_gap = input(\"Which gap do you like to start looking for ^-^? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee7672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearch Starting Points Suggester\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mInput is a selected research topic transformers, and its gap fe along with any user preferences (e.g., 'I want to build new architectures').\n",
      "Based on this input, provide a list of 3-5 actionable starting points for a beginner researcher.\n",
      "For each starting point, include:\n",
      "- **Area:** A concise title for the starting point.\n",
      "- **Description:** What this starting point entails.\n",
      "- **Actionable Steps:** Specific, concrete actions the researcher can take immediately (e.g., 'Search for recent papers on X', 'Explore open-source libraries for Y', 'Read foundational surveys on Z', 'Look for benchmark datasets for W').\n",
      "Consider different angles: theoretical, empirical, practical application, data-driven, model-building, etc.\n",
      "Output the suggestions in JSON format as defined by ResearchStartingPointsOutput.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearch Starting Points Suggester\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{\n",
      "  \"suggested_starting_points\": [\n",
      "    {\n",
      "      \"area\": \"Survey of Transformer Architectures for Specific Tasks\",\n",
      "      \"description\": \"Begin by understanding the current landscape of transformer architectures and their applications. Focus on a specific task (e.g., natural language processing, computer vision, time series forecasting) to narrow the scope.\",\n",
      "      \"actionable_steps\": [\n",
      "        \"Search for recent survey papers on transformer architectures in your chosen task area (e.g., 'A Survey of Transformer Architectures for Natural Language Processing').\",\n",
      "        \"Identify key architectural innovations and their impact on performance.\",\n",
      "        \"Analyze the strengths and weaknesses of different architectures for your chosen task.\",\n",
      "        \"Create a table summarizing key architectures, their characteristics, and their performance on benchmark datasets.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"area\": \"Exploring Transformer Limitations and Open Research Problems\",\n",
      "      \"description\": \"Identify the current limitations of transformer models, such as computational cost, data efficiency, or interpretability. This will help you pinpoint areas ripe for new research.\",\n",
      "      \"actionable_steps\": [\n",
      "        \"Read recent papers discussing the limitations of transformers (e.g., search for papers on 'Transformer efficiency', 'Transformer interpretability', or 'Transformer robustness').\",\n",
      "        \"Identify recurring themes and challenges in the literature.\",\n",
      "        \"Focus on a specific limitation that interests you and explore potential solutions.\",\n",
      "        \"Formulate research questions based on the identified limitations.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"area\": \"Developing a Novel Transformer-based Architecture\",\n",
      "      \"description\": \"If you're interested in building new architectures, start with a small, well-defined modification to an existing architecture.  Focus on a specific aspect, such as attention mechanisms or positional encoding.\",\n",
      "      \"actionable_steps\": [\n",
      "        \"Select a well-established transformer architecture as a baseline (e.g., BERT, ViT).\",\n",
      "        \"Identify a specific component to modify (e.g., the attention mechanism, the feed-forward network, the positional encoding).\",\n",
      "        \"Explore relevant research papers that propose modifications to that component.\",\n",
      "        \"Implement your modified architecture using a suitable deep learning framework (e.g., PyTorch, TensorFlow).\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"area\": \"Empirical Evaluation of Transformer Variants\",\n",
      "      \"description\": \"Compare the performance of different transformer architectures or variations on a specific task and dataset. This is a data-driven approach that can provide valuable insights.\",\n",
      "      \"actionable_steps\": [\n",
      "        \"Select a relevant benchmark dataset for your chosen task.\",\n",
      "        \"Choose several transformer architectures or variations to compare.\",\n",
      "        \"Implement the chosen architectures using a deep learning framework.\",\n",
      "        \"Train and evaluate the models using appropriate metrics.\",\n",
      "        \"Analyze the results and draw conclusions about the relative strengths and weaknesses of the different architectures.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"area\": \"Investigating the Interpretability of Transformers\",\n",
      "      \"description\": \"Focus on understanding how transformers make decisions. This is a crucial area for building trust and improving the reliability of these models.\",\n",
      "      \"actionable_steps\": [\n",
      "        \"Read survey papers on transformer interpretability techniques.\",\n",
      "        \"Explore different methods for visualizing attention weights or feature maps.\",\n",
      "        \"Apply these techniques to a specific transformer model and dataset.\",\n",
      "        \"Analyze the results and discuss their implications for understanding model behavior.\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ–‡ AgentOps: \u001b[34m\u001b[34mSession Replay for default.session trace: https://app.agentops.ai/sessions?trace_id=ec530ef96d76d2aa79906cd289bedb02\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'suggested_starting_points': [{'area': 'Survey of Transformer Architectures for Specific Tasks', 'description': 'Begin by understanding the current landscape of transformer architectures and their applications. Focus on a specific task (e.g., natural language processing, computer vision, time series forecasting) to narrow the scope.', 'actionable_steps': [\"Search for recent survey papers on transformer architectures in your chosen task area (e.g., 'A Survey of Transformer Architectures for Natural Language Processing').\", 'Identify key architectural innovations and their impact on performance.', 'Analyze the strengths and weaknesses of different architectures for your chosen task.', 'Create a table summarizing key architectures, their characteristics, and their performance on benchmark datasets.']}, {'area': 'Exploring Transformer Limitations and Open Research Problems', 'description': 'Identify the current limitations of transformer models, such as computational cost, data efficiency, or interpretability. This will help you pinpoint areas ripe for new research.', 'actionable_steps': [\"Read recent papers discussing the limitations of transformers (e.g., search for papers on 'Transformer efficiency', 'Transformer interpretability', or 'Transformer robustness').\", 'Identify recurring themes and challenges in the literature.', 'Focus on a specific limitation that interests you and explore potential solutions.', 'Formulate research questions based on the identified limitations.']}, {'area': 'Developing a Novel Transformer-based Architecture', 'description': \"If you're interested in building new architectures, start with a small, well-defined modification to an existing architecture.  Focus on a specific aspect, such as attention mechanisms or positional encoding.\", 'actionable_steps': ['Select a well-established transformer architecture as a baseline (e.g., BERT, ViT).', 'Identify a specific component to modify (e.g., the attention mechanism, the feed-forward network, the positional encoding).', 'Explore relevant research papers that propose modifications to that component.', 'Implement your modified architecture using a suitable deep learning framework (e.g., PyTorch, TensorFlow).']}, {'area': 'Empirical Evaluation of Transformer Variants', 'description': 'Compare the performance of different transformer architectures or variations on a specific task and dataset. This is a data-driven approach that can provide valuable insights.', 'actionable_steps': ['Select a relevant benchmark dataset for your chosen task.', 'Choose several transformer architectures or variations to compare.', 'Implement the chosen architectures using a deep learning framework.', 'Train and evaluate the models using appropriate metrics.', 'Analyze the results and draw conclusions about the relative strengths and weaknesses of the different architectures.']}, {'area': 'Investigating the Interpretability of Transformers', 'description': 'Focus on understanding how transformers make decisions. This is a crucial area for building trust and improving the reliability of these models.', 'actionable_steps': ['Read survey papers on transformer interpretability techniques.', 'Explore different methods for visualizing attention weights or feature maps.', 'Apply these techniques to a specific transformer model and dataset.', 'Analyze the results and discuss their implications for understanding model behavior.']}]}\n"
     ]
    }
   ],
   "source": [
    "class ResearchGapSection(BaseModel):\n",
    "    section: str\n",
    "    tips: str\n",
    "\n",
    "class ResearchGapOutput(BaseModel):\n",
    "    research_steps: List[ResearchGapSection] = Field(..., title=\"Research gap focused steps and tips\")\n",
    "\n",
    "research_gap_agent = Agent(\n",
    "    role=\"Research Gap Exploration Agent\",\n",
    "    goal=\"\\n\".join([\n",
    "        f\"Provide a detailed and clear set of specific research starting points based on the chosen {chosen_gap} in the {chosen_topic}.\",\n",
    "        \"Include practical and beginner-friendly tips for each step to help users start their research.\",\n",
    "        \"Focus on actionable tasks tied directly to the selected gap (e.g., watermarking, hallucination, bias).\",\n",
    "        \"Motivate users by giving confidence and clear direction.\"\n",
    "    ]),\n",
    "    backstory=\"Helps users dive into LLM research by breaking down complex gaps into simple, actionable steps.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "research_gap_task = Task(\n",
    "    description=\"\\n\".join([\n",
    "        f\"Input: the chosen research gap {chosen_gap} in the topic {chosen_topic} .\",\n",
    "        \"Output: a structured list of specific research steps with detailed tips for each step.\",\n",
    "        \"Goal: help beginners understand what to do first, what resources to use, and how to progress in a steps.\"\n",
    "    ]),\n",
    "    expected_output=\"JSON list of steps with detailed beginner tips.\",\n",
    "    output_json=ResearchGapOutput,\n",
    "    output_file=os.path.join(output_dir, \"step_research_gap.json\"),\n",
    "    agent=research_gap_agent,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Phase 2: Continue with remaining tasks\n",
    "second_crew = Crew(\n",
    "    name=\"PublishMate Crew - Phase 2\",\n",
    "    description=\"Suggest research starting points based on user-selected gap/topic.\",\n",
    "    agents=[\n",
    "        research_starting_points_agent,\n",
    "    ],\n",
    "    tasks=[\n",
    "        research_starting_points_task,\n",
    "    ],\n",
    ")\n",
    "\n",
    "second_result = second_crew.kickoff()\n",
    "print(second_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e74ee67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "query": "Efficient Transformer Training OR Large Model Training OR Model Parallelism published in 2023 OR 2024 OR 2025 site:arxiv.org OR site:ieeexplore.ieee.org OR site:springerlink.com OR site:sciencedirect.com",
  "follow_up_questions": null,
  "answer": null,
  "images": [],
  "results": [
    {
      "title": "Improving Automatic Parallel Training via Balanced Memory Workload ...",
      "url": "https://ieeexplore.ieee.org/document/10449463",
      "content": "Transformer models have emerged as the leading approach for achieving state-of-the-art performance across various application domains, serving as the foundation for advanced large-scale deep learning (DL) models. However, efficiently training these models across multiple GPUs remains a complex challenge due to the abundance of parallelism options. Existing DL systems either require manual",
      "score": 0.31821015,
      "raw_content": null
    },
    {
      "title": "Communication-Efficient Model Parallelism for Distributed In-Situ ...",
      "url": "https://ieeexplore.ieee.org/abstract/document/10546617",
      "content": "Transformer models have shown significant success in a wide range of tasks. Meanwhile, massive resources required by its inference prevent scenarios with resource-constrained devices from in-situ deployment, leaving a high threshold of integrating its advances. Observing that these scenarios, e.g. smart home of edge computing, are usually comprise a rich set of trusted devices with untapped",
      "score": 0.23291197,
      "raw_content": null
    },
    {
      "title": "A comprehensive survey on applications of transformers for deep ...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S0957417423031688",
      "content": "The advantages of the Transformer model have inspired deep learning researchers to explore its potential for various tasks in different fields of application (Ren, Li, & Liu, 2023), leading to numerous research papers and the development of Transformer-based models for a range of tasks in the field of artificial intelligence (Reza et al., 2022",
      "score": 0.13793866,
      "raw_content": null
    },
    {
      "title": "Joint Dynamic Data and Model Parallelism for Distributed Training of ...",
      "url": "https://ieeexplore.ieee.org/document/10767392",
      "content": "Distributed training of deep neural networks (DNNs) suffers from efficiency declines in dynamic heterogeneous environments, due to the resource wastage brought by the straggler problem in data parallelism (DP) and pipeline bubbles in model parallelism (MP). Additionally, the limited resource availability requires a trade-off between training performance and long-term costs, particularly in",
      "score": 0.10787861,
      "raw_content": null
    },
    {
      "title": "A comprehensive survey on applications of transformers for deep ...",
      "url": "https://www.sciencedirect.com/science/article/pii/S0957417423031688",
      "content": "Transformers are Deep Neural Networks (DNN) that utilize a self-attention mechanism to capture contextual relationships within sequential data. Unlike\u2026",
      "score": 0.04569639,
      "raw_content": null
    }
  ],
  "response_time": 2.62
}
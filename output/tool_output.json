{
  "query": "Transformer Applications beyond NLP 2023-2025 arxiv ieee springer",
  "follow_up_questions": null,
  "answer": null,
  "images": [],
  "results": [
    {
      "title": "Transformers Beyond NLP: Expanding Horizons in Machine Learning - SSRN",
      "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5110547",
      "content": "Transformers Beyond NLP: Expanding Horizons in Machine Learning by Srikanth Kamatala, Anil Kumar Jonnalagadda, Prudhvi Naayini :: SSRN Top Papers Top Papers Kamatala, Srikanth and Jonnalagadda, Anil Kumar and Naayini, Prudhvi, Transformers Beyond NLP: Expanding Horizons in Machine Learning (January 21, 2025). We use cookies to help provide and enhance our service and tailor content. To learn more, visit Cookie Settings. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. #### Functional Cookies - [x] Functional Cookies  If you do not allow these cookies then some or all of these services may not function properly. These cookies may be set through our site by our advertising partners.",
      "score": 0.4431913,
      "raw_content": null
    },
    {
      "title": "Advancements in Natural Language Processing: Exploring Transformer ...",
      "url": "https://arxiv.org/abs/2503.20227",
      "content": "[2503.20227] Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding Title:Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding View a PDF of the paper titled Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding, by Tianhao Wu and 1 other authors Comments:This paper has been accepted by the 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA 2025)Subjects:Computation and Language (cs.CL); Artificial Intelligence (cs.AI)Cite as:arXiv:2503.20227 [cs.CL]\u00a0(or arXiv:2503.20227v1 [cs.CL] for this version)\u00a0https://doi.org/10.48550/arXiv.2503.20227Focus to learn morearXiv-issued DOI via DataCite (pending registration) View a PDF of the paper titled Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding, by Tianhao Wu and 1 other authors Bibliographic Explorer Toggle Connected Papers Toggle Links to Code Toggle Links to Code Toggle",
      "score": 0.42062396,
      "raw_content": null
    },
    {
      "title": "Transformer Model Applications: A Comprehensive Survey and ... - Springer",
      "url": "https://link.springer.com/chapter/10.1007/978-981-97-4533-3_3",
      "content": "Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, Cistac P, Rault T, Louf R, Funtowicz M et al (2020) Transformers: state-of-the-art natural language processing. Lewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, Stoyanov V, Zettlemoyer L (2019) Bart: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Dong L, Yang N, Wang W, Wei F, Liu X, Wang Y, Gao J, Zhou M, Hon HW (2019) Unified language model pre-training for natural language understanding and generation. Bao H, Dong L, Wei F, Wang W, Yang N, Liu X, Wang Y, Gao J, Piao S, Zhou M et al (2020) Unilmv2: Pseudo-masked language models for unified language model pre-training.",
      "score": 0.34640914,
      "raw_content": null
    },
    {
      "title": "Transformer Models in Natural Language Processing: A ... - Springer",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-81308-5_42",
      "content": "Transformer-based pre-trained language models are advanced machine learning models that understand and produce human language. Chen, A., Yu, Z., Yang, X., Guo, Y., Bian, J., Wu, Y.: Contextualized medication information extraction using Transformer-based deep learning architectures. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional transformers for language understanding (2019). Li, R., Jiang, Z., Wang, L., Lu, X., Zhao, M., Chen, D.: Enhancing Transformer-based language models with commonsense representations for knowledge-driven machine comprehension. Chen, A., Yu, Z., Yang, X., Guo, Y., Bian, J., Wu, Y.: Contextualized medication information extraction using Transformer-based deep learning architectures. Li, R., Jiang, Z., Wang, L., Lu, X., Zhao, M., Chen, D.: Enhancing Transformer-based language models with commonsense representations for knowledge-driven machine comprehension.",
      "score": 0.32082182,
      "raw_content": null
    },
    {
      "title": "PDF",
      "url": "https://link.springer.com/content/pdf/10.1007/s12559-025-10470-w.pdf",
      "content": "Transformer-Encoder SSRAN [57] 2023 Y \u2212 77.90 BiSLU [58] 2023 Y \u2212 81.50 Transformer-Decoder EN-Llama-2 [59] 2024 Y \u2212 80.60 EN-Mistral [59] 2024 Y \u2212 82.40 Transformer & RNN Uni-MIS [60] 2024 Y \u2212 78.50 MixSNIPS LSTM GL-GIN [55] 2021 N \u2212 95.60 SDJN [56] 2022 N \u2212 96.50 Transformer-Encoder SSRAN [57] 2023 Y \u2212 98.40 BiSLU [58] 2023 Y",
      "score": 0.068793684,
      "raw_content": null
    }
  ],
  "response_time": 1.65
}
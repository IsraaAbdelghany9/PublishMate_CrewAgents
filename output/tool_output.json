{
  "query": "Interpretability and Explainability of Transformers published in 2023 OR 2024 OR 2025 in IEEE, Springer, Elsevier, arXiv",
  "follow_up_questions": null,
  "answer": null,
  "images": [],
  "results": [
    {
      "title": "Enforcing Interpretability in Time Series Transformers: A Concept ...",
      "url": "https://arxiv.org/abs/2410.06070",
      "content": "There has been a recent push of research on Transformer-based models for long-term time series forecasting, even though they are inherently difficult to interpret and explain. While there is a large body of work on interpretability methods for various domains and architectures, the interpretability of Transformer-based forecasting models remains largely unexplored. To address this gap, we",
      "score": 0.31738445,
      "raw_content": null
    },
    {
      "title": "[2304.10557] An Introduction to Transformers - arXiv.org",
      "url": "https://arxiv.org/abs/2304.10557",
      "content": ">cs> arXiv:2304.10557  arXiv:2304.10557 (cs)  View a PDF of the paper titled An Introduction to Transformers, by Richard E. Subjects:Machine Learning (cs.LG); Artificial Intelligence (cs.AI) Cite as:arXiv:2304.10557 [cs.LG] (or arXiv:2304.10557v5 [cs.LG] for this version) From: Richard Turner [view email]  View a PDF of the paper titled An Introduction to Transformers, by Richard E. cs [x] Bibliographic Explorer Toggle  [x] Connected Papers Toggle  [x] Litmaps Toggle  [x] scite.ai Toggle  [x] alphaXiv Toggle  [x] Links to Code Toggle  [x] DagsHub Toggle  [x] GotitPub Toggle  [x] Huggingface Toggle  [x] Links to Code Toggle  [x] ScienceCast Toggle  [x] Replicate Toggle  [x] Spaces Toggle  [x] Spaces Toggle  [x] Link to Influence Flower  [x] Core recommender toggle  [x] IArxiv recommender toggle  Learn more about arXivLabs.",
      "score": 0.18103175,
      "raw_content": null
    },
    {
      "title": "[2407.02646] A Practical Review of Mechanistic Interpretability for ...",
      "url": "https://arxiv.org/abs/2407.02646",
      "content": "Mechanistic interpretability (MI) is an emerging sub-field of interpretability that seeks to understand a neural network model by reverse-engineering its internal computations. Recently, MI has garnered significant attention for interpreting transformer-based language models (LMs), resulting in many novel insights yet introducing new challenges. However, there has not been work that",
      "score": 0.15614378,
      "raw_content": null
    },
    {
      "title": "Title: A Tale of Two Imperatives: Privacy and Explainability - arXiv.org",
      "url": "https://arxiv.org/abs/2412.20798",
      "content": "arXivLabs: experimental projects with community collaborators. arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.",
      "score": 0.08933553,
      "raw_content": null
    },
    {
      "title": "SAMformer: Unlocking the Potential of Transformers in Time Series ...",
      "url": "https://arxiv.org/abs/2402.10198",
      "content": "Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite",
      "score": 0.05382461,
      "raw_content": null
    }
  ],
  "response_time": 2.72
}
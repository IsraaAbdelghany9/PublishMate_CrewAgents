{
  "query": "Multimodal RAG papers 2023-2025",
  "follow_up_questions": null,
  "answer": null,
  "images": [],
  "results": [
    {
      "title": "Paper Collections | llm-lab-org/Multimodal-RAG-Survey | DeepWiki",
      "url": "https://deepwiki.com/llm-lab-org/Multimodal-RAG-Survey/6-paper-collections",
      "content": "Sources: README.md 425-432 README.md 434-473 Publication Trends and Statistics. The paper collections reflect the rapid growth and evolution of Multimodal RAG research. The repository includes papers from 2021 to 2025, with a significant increase in publications observed in 2023-2024, particularly in areas like video-centric retrieval and multimodal fusion mechanisms.",
      "score": 0.85097736,
      "raw_content": null
    },
    {
      "title": "ICLR Poster MMed-RAG: Versatile Multimodal RAG System for Medical ...",
      "url": "https://iclr.cc/virtual/2025/poster/28145",
      "content": "Select Year: (2025) 2025 2024 2023 2022 2021 2020 2019 2018 2017 ... In this paper, we propose a versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an adaptive retrieved contexts selection, and a provable RAG-based preference fine-tuning",
      "score": 0.79369855,
      "raw_content": null
    },
    {
      "title": "[2504.12330] HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval ...",
      "url": "https://arxiv.org/abs/2504.12330",
      "content": "View a PDF of the paper titled HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation, by Pei Liu and 6 other authors We present HM-RAG, a novel Hierarchical Multi-agent Multimodal RAG framework that pioneers collaborative intelligence for dynamic knowledge synthesis across structured, unstructured, and graph-based data. View a PDF of the paper titled HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation, by Pei Liu and 6 other authors Bibliographic Explorer Toggle Connected Papers Toggle Litmaps Toggle alphaXiv Toggle Links to Code Toggle DagsHub Toggle GotitPub Toggle Links to Code Toggle ScienceCast Toggle Replicate Toggle",
      "score": 0.48718598,
      "raw_content": null
    },
    {
      "title": "Multimodal RAG Survey",
      "url": "https://multimodalrag.github.io/",
      "content": "Multimodal RAG extends traditional Retrieval-Augmented Generation (RAG) frameworks by incorporating diverse data types, such as text, images, audio, and video, from external knowledge sources. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. The typical Multimodal RAG pipeline involves several key stages, alongside crucial innovations in training and robustness, to effectively integrate diverse data types for enhanced generation: Enhancing Reasoning, Alignment, and Retrieval: Developing better compositional reasoning across modalities, improving entity-aware retrieval, mitigating retrieval biases (e.g., position sensitivity, redundancy), exploring knowledge graphs, and creating truly unified embedding spaces for direct multimodal search. title={Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation}, ",
      "score": 0.46584973,
      "raw_content": null
    },
    {
      "title": "GitHub - HuggingAGI/AwesomeRAGPapers: A curated collection of ...",
      "url": "https://github.com/HuggingAGI/AwesomeRAGPapers",
      "content": "A curated collection of influential surveys and papers on Retrieval-Augmented Generation (RAG), covering frameworks, evaluations, multi-modal extensions, and domain-specific applications. - GitHub - HuggingAGI/AwesomeRAGPapers: A curated collection of influential surveys and papers on Retrieval-Augmented Generation (RAG), covering frameworks, evaluations, multi-modal extensions, and domain",
      "score": 0.4387136,
      "raw_content": null
    }
  ],
  "response_time": 2.07
}
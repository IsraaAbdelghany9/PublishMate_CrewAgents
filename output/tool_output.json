{
  "query": "Transformer Applications beyond NLP 2023-2025 arxiv ieee springer",
  "follow_up_questions": null,
  "answer": null,
  "images": [],
  "results": [
    {
      "title": "Advancements in Natural Language Processing: Exploring Transformer ...",
      "url": "https://arxiv.org/abs/2503.20227",
      "content": "[2503.20227] Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding Title:Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding View a PDF of the paper titled Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding, by Tianhao Wu and 1 other authors Comments:This paper has been accepted by the 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA 2025)Subjects:Computation and Language (cs.CL); Artificial Intelligence (cs.AI)Cite as:arXiv:2503.20227 [cs.CL]\u00a0(or arXiv:2503.20227v1 [cs.CL] for this version)\u00a0https://doi.org/10.48550/arXiv.2503.20227Focus to learn morearXiv-issued DOI via DataCite (pending registration) View a PDF of the paper titled Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding, by Tianhao Wu and 1 other authors Bibliographic Explorer Toggle Connected Papers Toggle Links to Code Toggle Links to Code Toggle",
      "score": 0.42090955,
      "raw_content": null
    },
    {
      "title": "Transformer Model Applications: A Comprehensive Survey and ... - Springer",
      "url": "https://link.springer.com/chapter/10.1007/978-981-97-4533-3_3",
      "content": "Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, Cistac P, Rault T, Louf R, Funtowicz M et al (2020) Transformers: state-of-the-art natural language processing. Lewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, Stoyanov V, Zettlemoyer L (2019) Bart: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Dong L, Yang N, Wang W, Wei F, Liu X, Wang Y, Gao J, Zhou M, Hon HW (2019) Unified language model pre-training for natural language understanding and generation. Bao H, Dong L, Wei F, Wang W, Yang N, Liu X, Wang Y, Gao J, Piao S, Zhou M et al (2020) Unilmv2: Pseudo-masked language models for unified language model pre-training.",
      "score": 0.34640914,
      "raw_content": null
    },
    {
      "title": "A Comprehensive Survey on Applications of Transformers for Deep ...",
      "url": "https://arxiv.org/abs/2306.07303",
      "content": "[2306.07303] A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks >cs> arXiv:2306.07303  arXiv:2306.07303 (cs)  View a PDF of the paper titled A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks, by Saidul Islam and 6 other authors View a PDF of the paper titled A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks, by Saidul Islam and 6 other authors [x] Bibliographic Explorer Toggle  [x] Connected Papers Toggle  [x] Litmaps Toggle  [x] scite.ai Toggle  [x] alphaXiv Toggle  [x] Links to Code Toggle  [x] DagsHub Toggle  [x] GotitPub Toggle  [x] Huggingface Toggle  [x] Links to Code Toggle  [x] ScienceCast Toggle  [x] Replicate Toggle  [x] Spaces Toggle  [x] Spaces Toggle  [x] Core recommender toggle  [x] IArxiv recommender toggle ",
      "score": 0.32649738,
      "raw_content": null
    },
    {
      "title": "Transformer Models in Natural Language Processing: A ... - Springer",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-81308-5_42",
      "content": "Transformer-based pre-trained language models are advanced machine learning models that understand and produce human language. Chen, A., Yu, Z., Yang, X., Guo, Y., Bian, J., Wu, Y.: Contextualized medication information extraction using Transformer-based deep learning architectures. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional transformers for language understanding (2019). Li, R., Jiang, Z., Wang, L., Lu, X., Zhao, M., Chen, D.: Enhancing Transformer-based language models with commonsense representations for knowledge-driven machine comprehension. Chen, A., Yu, Z., Yang, X., Guo, Y., Bian, J., Wu, Y.: Contextualized medication information extraction using Transformer-based deep learning architectures. Li, R., Jiang, Z., Wang, L., Lu, X., Zhao, M., Chen, D.: Enhancing Transformer-based language models with commonsense representations for knowledge-driven machine comprehension.",
      "score": 0.32104528,
      "raw_content": null
    },
    {
      "title": "Advancements in Natural Language Processing: Exploring Transformer ...",
      "url": "https://arxiv.org/pdf/2503.20227",
      "content": "This paper explores the advancements in transformer models, such as BERT and GPT, focusing on their superior performance in text understanding tasks compared to traditional methods like recurrent neural networks (RNNs) [1]. BERT Text Understanding Bidirectional Context GLUE >80% Accuracy T5 Question Answering Near-Human Performance SQuAD F1 >90% CA-BERT Multi-Turn Chat Enhanced Context Awareness Superior Context Classification GPT-3 Text Generation Scalability (175B Parameters) High Coherence in Generation TABLE I. Models like BERT and GPT have demonstrated remarkable capabilities in capturing contextual nuances, handling long-range dependencies, and adapting to a wide range of tasks, as evidenced by their superior performance on benchmarks such as GLUE (with accuracy exceeding 80%) and SQuAD (with F1 scores above 90%).",
      "score": 0.13401501,
      "raw_content": null
    }
  ],
  "response_time": 1.67
}
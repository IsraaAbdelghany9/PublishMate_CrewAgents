{
  "query": "Improving Transformer Robustness and Generalization 2023-2025 arxiv ieee springer",
  "follow_up_questions": null,
  "answer": null,
  "images": [],
  "results": [
    {
      "title": "Optimal Control for Transformer Architectures: Enhancing Generalization ...",
      "url": "https://arxiv.org/abs/2505.13499",
      "content": ">cs> arXiv:2505.13499  arXiv:2505.13499 (cs)  View a PDF of the paper titled Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency, by Kelvin Kan and 5 other authors Subjects:Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC) View a PDF of the paper titled Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency, by Kelvin Kan and 5 other authors [x] Bibliographic Explorer Toggle  [x] Connected Papers Toggle  [x] Litmaps Toggle  [x] scite.ai Toggle  [x] alphaXiv Toggle  [x] Links to Code Toggle  [x] DagsHub Toggle  [x] GotitPub Toggle  [x] Huggingface Toggle  [x] Links to Code Toggle  [x] ScienceCast Toggle  [x] Replicate Toggle  [x] Spaces Toggle  [x] Spaces Toggle  [x] Core recommender toggle  [x] IArxiv recommender toggle ",
      "score": 0.5342973,
      "raw_content": null
    },
    {
      "title": "[2311.17400] Improving the Robustness of Transformer-based Large ...",
      "url": "https://arxiv.org/abs/2311.17400",
      "content": "arXiv smileybones ## arXiv Is Hiring a DevOps Engineer arXiv Is Hiring a DevOps Engineer arxiv logo arXiv logo Cornell University Logo ### References & Citations ## BibTeX formatted citation BibSonomy logo # Bibliographic and Citation Tools # Recommenders and Search Tools # arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? arXiv Operational Status   ",
      "score": 0.37614232,
      "raw_content": null
    },
    {
      "title": "Improving Transformer Inference Through Optimized ... - IEEE Xplore",
      "url": "https://ieeexplore.ieee.org/document/10738457",
      "content": "Improving Transformer Inference Through Optimized Nonlinear Operations With Quantization-Approximation-Based Strategy | IEEE Journals & Magazine | IEEE Xplore Published in:IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems ( Volume: 44, Issue: 4, April 2025)  Image 4: Contact IEEE to Subscribe IEEE Personal Account About IEEE _Xplore_ | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A public charity, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. ### IEEE Account About IEEE _Xplore_ A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.",
      "score": 0.37579864,
      "raw_content": null
    },
    {
      "title": "Self-Improving Transformers Overcome Easy-to-Hard and Length ...",
      "url": "https://arxiv.org/abs/2502.01612",
      "content": ">cs> arXiv:2502.01612  arXiv:2502.01612 (cs)  Title:Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges View a PDF of the paper titled Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges, by Nayoung Lee and 4 other authors Cite as:arXiv:2502.01612 [cs.LG] (or arXiv:2502.01612v2 [cs.LG] for this version) View a PDF of the paper titled Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges, by Nayoung Lee and 4 other authors [x] Bibliographic Explorer Toggle  [x] Connected Papers Toggle  [x] Litmaps Toggle  [x] scite.ai Toggle  [x] alphaXiv Toggle  [x] Links to Code Toggle  [x] DagsHub Toggle  [x] GotitPub Toggle  [x] Huggingface Toggle  [x] Links to Code Toggle  [x] ScienceCast Toggle  [x] Replicate Toggle  [x] Spaces Toggle  [x] Spaces Toggle  [x] Core recommender toggle  [x] IArxiv recommender toggle ",
      "score": 0.27119908,
      "raw_content": null
    },
    {
      "title": "Optimal Control for Transformer Architectures: Enhancing Generalization ...",
      "url": "https://arxiv.org/pdf/2505.13499",
      "content": "The model inserts a given Transformer into a continuous-time architecture and incorporates an optimal transport (OT)-regularization into the training objective, which are motivated from the theoretical analysis using optimal control theory in Section 4. The unregularized OT-Transformer improves the test accuracy to 96.8%, although it uses a much smaller model architecture. The N-ODE Transformers with slight and standard regularization report a test accuracy of 83.6% and 83.9%, respectively, which are not better than the baseline model. The baseline architecture uses a Transformer 33 Table 6: Number of parameters for the Transformer blocks, best and final test accuracies (with standard deviation) across five trials for for the sentiment analysis experiment.",
      "score": 0.23035698,
      "raw_content": null
    }
  ],
  "response_time": 1.7
}